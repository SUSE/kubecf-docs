[{"body":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy and track CF Deployments, consuming directly also BOSH Releases, among Kubernetes Native components.\nWhere should I go next?  Getting Started: Get started with KubeCF Tutorials: Check out some tutorials!  ","excerpt":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy …","ref":"https://kubecf.suse.dev/docs/overview/","title":"Overview"},{"body":" The make cats target starts a run of the Cloud Foundry Acceptance Tests.\nSee also the entire set of available tests.\nLimiting test suites To limit the test groups to run, set the BOSH property acceptance_tests.include as documented. To do so, adjust the properties key in values.yaml to specify the groups desired. For example:\nproperties:acceptance-tests:acceptance-tests:acceptance_tests:include:\u0026#34;+docker,-ssh\u0026#34; Note that this is an example of how to use the second kind of customization feature noted in the main README.\n","excerpt":"The make cats target starts a run of the Cloud Foundry Acceptance Tests.\nSee also the entire set of …","ref":"https://kubecf.suse.dev/docs/reference/tests/","title":"Tests"},{"body":" Prerequisites  A Kubernetes cluster Presence of a default storage class (provisioner). For use with a diego-based kubecf (default), a node OS with XFS support.  For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.  Helm 3  Installation KubeCF is packaged as an Helm chart.\nCheck the release page for a full list of the official releases.\nNightly builds can be found on the KubeCF public s3 bucket.\nThere are two assets for each release: one is the \u0026ldquo;kubecf-bundle\u0026rdquo; which contains the KubeCF chart along with the cf-operator version required to deploy it, while the standalone chart containts only the KubeCF chart.\nThe standalone chart (kubecf.tgz) contains a Metadata.yaml file which indicates the version of the cf-operator to install for a successfull deployment.\nTry it out! ","excerpt":"Prerequisites  A Kubernetes cluster Presence of a default storage class (provisioner). For use with …","ref":"https://kubecf.suse.dev/docs/getting-started/","title":"Getting Started"},{"body":" Before you begin This guideline will provide a quick way to deploy KubeCF with Kind and should be used only for evaluation purposes.\nHere is a list of all the tools and versions used when creating these instructions:\n\u0026gt; kind version kind v0.7.0 go1.13.6 darwin/amd64  \u0026gt; helm version version.BuildInfo{Version:\u0026quot;v3.1.1\u0026quot;, GitCommit:\u0026quot;afe70585407b420d0097d07b21c47dc511525ac8\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, GoVersion:\u0026quot;go1.13.8\u0026quot;}  \u0026gt; docker version Client: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:22:34 2019 OS/Arch: darwin/amd64 Experimental: false  \u0026gt; kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;17\u0026quot;, GitVersion:\u0026quot;v1.17.3\u0026quot;, GitCommit:\u0026quot;06ad960bfd03b39c8310aaf92d1e7c12ce618213\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2020-02-13T18:08:14Z\u0026quot;, GoVersion:\u0026quot;go1.13.8\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;darwin/amd64\u0026quot;}  \u0026gt; cf version cf version 6.46.1+4934877ec.2019-08-23z  Installing Kind To install Kind please follow the official instructions here.\nCreate a Kind cluster Run the following commands:\n\u0026gt; export KUBECONFIG=kubeconfig-kubecf \u0026gt; kind create cluster --name kubecf \u0026gt; kubectl cluster-info --context kind-kubecf  Download cf-operator and KubeCF Helm charts Download the latest release bundle (kubecf-bundle-vX.X.X.tgz) to a local directory and extract it:\n\u0026gt; curl -s https://api.github.com/repos/cloudfoundry-incubator/kubecf/releases/latest \\ | grep -oP '\u0026quot;browser_download_url\u0026quot;: \u0026quot;\\K(.*)kubecf-bundle(.*)(?=\u0026quot;)' \\ | wget -qi - \u0026gt; tar xf kubecf-bundle-v*.tgz  The archive contains cf-operator and KubeCF Helm charts (cf-operator.tgz and kubecf_release.tgz respectively).\nAll releases can be found on KubeCF releases page.\nInstalling cf-operator Before we can deploy the cf-operator we need to create the namespace:\n\u0026gt; kubectl create ns cfo  and after we can install by running the helm command:\n\u0026gt; helm install cf-operator \\ --namespace cfo \\ --set \u0026quot;global.singleNamespace.name=kubecf\u0026quot; \\ ./cf-operator.tgz  Notes:\n The singleNamespace property is set to watch the kubecf namespace for changes cf-operator version may differ between KubeCF versions  Check if the pods are up and running before moving to the next section:\n\u0026gt; kubectl get pods -n cfo  Installing KubeCF First let\u0026rsquo;s get Kind node IP address:\nnode_ip=$(kubectl get node kubecf-control-plane \\ --output jsonpath='{ .status.addresses[?(@.type == \u0026quot;InternalIP\u0026quot;)].address }')  and then set the properties correctly:\ncat \u0026lt;\u0026lt; _EOF_ \u0026gt; values.yaml system_domain: ${node_ip}.nip.io services: router: externalIPs: - ${node_ip} kube: service_cluster_ip_range: 0.0.0.0/0 pod_cluster_ip_range: 0.0.0.0/0 _EOF_  On this example, we will use Diego instead Eirini but you can easily switch by adding the following lines into your values.yaml file:\n... features: eirini: enabled: true ...  and, you need to trust the kubernetes root CA on the kind docker container:\n\u0026gt; docker exec -it \u0026quot;kubecf-control-plane\u0026quot; bash -c 'cp /etc/kubernetes/pki/ca.crt /etc/ssl/certs/ \u0026amp;\u0026amp; \\ update-ca-certificates \u0026amp;\u0026amp; \\ (systemctl list-units | grep containerd \u0026gt; /dev/null \u0026amp;\u0026amp; systemctl restart containerd)'  the values.yaml file should be similiar to the snippet:\nsystem_domain: 172.17.0.3.nip.io services: router: loadBalancerIP: - 172.17.0.3 kube: service_cluster_ip_range: 0.0.0.0/0 pod_cluster_ip_range: 0.0.0.0/0  Now is time to install KubeCF by running the helm command:\n\u0026gt; helm install kubecf \\ --namespace kubecf \\ --values values.yaml \\ ./kubecf_release.tgz  Notes:\n the namespace property value is the same as cf-operator singleNamespace one  Be aware that it takes a couple of minutes to see the pods showing up on the kubecf namespace and the installation process may take 20-25 minutes depending on your internet connection speed.\nRun the following command to watch the pods progress:\n\u0026gt; watch kubectl get pods -n kubecf  After all the pods are running you can check by running the cf cli command:\n\u0026gt; cf api --skip-ssl-validation api.172.17.0.3.nip.io  get the admin password:\n\u0026gt; admin_pass=$(kubectl get secret --namespace kubecf \\ var-cf-admin-password \\ -o jsonpath='{.data.password}' | base64 --decode)  and login with: cf auth admin \u0026quot;${admin_pass}\u0026quot;\nWhat\u0026rsquo;s next After the deployment finishes with success it\u0026rsquo;s time to give it a try by pushing an app using the cf-push cli command.\nCleaning up \u0026gt; kind delete cluster --name kubecf  ","excerpt":"Before you begin This guideline will provide a quick way to deploy KubeCF with Kind and should be …","ref":"https://kubecf.suse.dev/docs/tutorials/deploy-kind/","title":"Deploy KubeCF on Kind"},{"body":" KubeCF allows to leverage the Kubernetes Cluster available StorageClass. When Eirini is being enabled, two additional components are deployed, the eirini-persi-extension and eirini-persi-broker.\nThe broker must be configured in order to use the storageclass to provide support for persistence data for applications pushed to CloudFoundry with Eirini.\nYou can set up default plans that are applied to the broker during deployment.\nTo configure the broker, in the KubeCF values add and adjust as needed:\neirinix:persi-broker:service-plans:-id:defaultname:\u0026#34;default\u0026#34;description:\u0026#34;Existing default storage class\u0026#34;kube_storage_class:\u0026#34;default\u0026#34;# Storageclass used for provisioningfree:truedefault_size:\u0026#34;1Gi\u0026#34;# Default size of generated PVCdefault_access_mode:\u0026#34;ReadWriteOnly\u0026#34;# Here you can tweak the default access mode for new PVCs Setup eirini-persi-broker with KubeCF In this section we will see how to setup the eirini-persi-broker on a KubeCF deployment.\nGet the broker password After deploying KubeCF the broker password should be automatically generated (the example assumes you have deployed KubeCF in the kubecf namespace):\n$\u0026gt; BROKER_PASS=$(kubectl get secrets -n kubecf -o json persi-broker-auth-password | jq -r '.data.\u0026quot;password\u0026quot;' | base64 -d)  Create the service broker With cf-cli (you must be logged in) let\u0026rsquo;s add the broker to our cf instance:\n$\u0026gt; cf create-service-broker eirini-persi admin $BROKER_PASS http://eirini-persi-broker:8999  Enable the service for the space Let\u0026rsquo;s enable the service (here, doing it globally):\n$\u0026gt; cf enable-service-access eirini-persi Enabling access to all plans of service eirini-persi for all orgs as admin... OK  Check everything is ok:\n$\u0026gt; cf service-brokers Getting service brokers as admin... name url eirini-persi http://eirini-persi-broker:8999 If eirini-persi is showed among the list of the available brokers, it means that it is configured and plans supplied during deployment are available to be consumed.\nNow the broker should be available in the marketplace, list all the broker services:\n$\u0026gt; cf marketplace Getting services from marketplace in org system / space tmp as admin... OK service plans description eirini-persi default Eirini persistence broker  List the plans available from the broker (output might differ):\n$\u0026gt; cf marketplace -s eirini-persi Getting service plan information for service eirini-persi as admin... OK service plan description free or paid default Eirini persistence broker free  Using the Eirini-persi broker To verify that everything works correctly it is possible to check after creating a service that the persistent volume claims are present in the Kubernetes cluster.\nCreate a service eirini-persi-broker will create PVCs associated to the services that are created:\n$\u0026gt; cf create-service eirini-persi default eirini-persi-1 Creating service instance eirini-persi-1 in org system / space tmp as admin... OK  In this way, we can associate the service to an app, which will actually link and attach a PVC to it.\nList all the PersistentVolumeClaim and verify that a new one was created:\n$\u0026gt; kubectl get pvc -n eirini NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE f9d4b977-0b9e-4fcb-a92e-652bc77dd7e7 Bound pvc-9960cc11-7895-11e9-afac-024267c84a6b 1Gi RWO persistent 6s  To create a service with a custom capacity, you can define the quota when creating the service.\nE.g. create a service providing a volume with 20M of quota:\n$\u0026gt; cf create-service eirini-persi default eirini-persi-1-20M-quota -c '{\u0026quot;size\u0026quot;: \u0026quot;20M\u0026quot;}'  E.g. To create a PVC which has a different access mode:\n$\u0026gt; cf create-service eirini-persi default eirini-persi-1-rwm -c '{\u0026quot;access_mode\u0026quot;: \u0026quot;ReadWriteMany\u0026quot;}'  Bind volumes to a Cloud Foundry application List the available services:\n$\u0026gt; cf services Getting services in org system / space tmp as admin... name service plan bound apps last operation eirini-persi-1 eirini-persi default dizzylizard create succeeded  Let\u0026rsquo;s associate the service to an application (in this case dizzylizard) using eirini-persi-1:\n$\u0026gt; cf bind-service dizzylizard eirini-persi-1 Binding service eirini-persi-1 to app dizzylizard in org system / space tmp as admin... OK  Restage our app so our change takes effect:\n$\u0026gt; cf restage dizzylizard Restaging app dizzylizard in org system / space tmp as admin...  Access the Volume Once the service is associated, the application can access to the data of the volume mount by reading the mounted path inside the VCAP_SERVICES environment variable.\nAn example of VCAP_SERVICES generated by eirini-persi-broker:\n{\u0026#34;eirini-persi\u0026#34;: [\t{ \u0026#34;credentials\u0026#34;: { \u0026#34;volume_id\u0026#34;: \u0026#34;the-volume-id\u0026#34; }, \u0026#34;label\u0026#34;: \u0026#34;eirini-persi\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;my-instance\u0026#34;, \u0026#34;plan\u0026#34;: \u0026#34;hostpath\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;erini\u0026#34;, \u0026#34;kubernetes\u0026#34;, \u0026#34;storage\u0026#34; ], \u0026#34;volume_mounts\u0026#34;: [ { \u0026#34;container_dir\u0026#34;: \u0026#34;/var/vcap/data/de847d34-bdcc-4c5d-92b1-cf2158a15b47\u0026#34;, \u0026#34;device_type\u0026#34;: \u0026#34;shared\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;rw\u0026#34; } ] } ] } You can refer to the Cloud Foundry documentation regarding of how to access to the Volume Service, with the difference that the eirini broker will create services with the id eirini-persi.\nSee also  https://github.com/SUSE/scf/wiki/Persistence-with-Eirini-in-SCF  ","excerpt":"KubeCF allows to leverage the Kubernetes Cluster available StorageClass. When Eirini is being …","ref":"https://kubecf.suse.dev/docs/tutorials/eirini_persi/","title":"Eirini Persistence broker"},{"body":" Required details  External Kubernetes IP UAA endpoint url CF admin username and password CF API url  Prepare Stratos setup config Replace \u0026lt;EXTERNAL_IP\u0026gt; by the external ip[s] of your cluster, adapt the port if required and store everything in a file, for example stratos.yml.\nconsole:service:externalIPs:[\u0026#34;\u0026lt;EXTERNAL_IP\u0026gt;\u0026#34;]servicePort:8443 Deploy Stratos Add the SUSE Kubernetes charts repo and deploy Stratos. Instead of the SUSE repo the Stratos chart can also be referenced from https://github.com/cloudfoundry/stratos/releases.\nhelm repo add suse https://kubernetes-charts.suse.com/ helm install --namespace stratos --name stratos --values ./stratos.yml suse/console Initial setup of Stratos As soon as Stratos is deployed connect to https://: and follow the steps.\nProvide your UAA Endpoint url, for example: https://uaa. (Make sure that there is no leading Slash at the end, otherwise UAA will ignore the authentication request.)\nThe client id is cf and the client secret stays empty.\nThe admin username is admin and password is the Cloudfoundry admin password. If you do not know the admin password yet, because for example it was autogenerated during the initial deployment of kubecf, it can be extracted with this command:\necho $(kubectl get secrets -n kubecf kubecf.var-cf-admin-password -o json | jq -r .data.password | base64 -d) The next step defines which uaa group specifies Stratos admins and does not need to be changed.\nAfter the initial setup Cloudfoundry can be added with the api endpoint url and the same cloudfoundry admin credentials.\n","excerpt":"Required details  External Kubernetes IP UAA endpoint url CF admin username and password CF API url …","ref":"https://kubecf.suse.dev/docs/tutorials/deploy-stratos/","title":"Deploy Stratos next to KubeCF"},{"body":" Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The Image Modify Kubecf to Use the New Image  Integrating the Release in Kubecf  BPM Operation Files  Testing With Kubecf  Preparing the Release Image BOSH release authors who want to test their development code with the Quarks operator need to build a Docker image from their release. This can be done with fissile. Afterwards, upload the image to a cluster for testing it, e.g. with Kubecf.\nBuilding a Docker Image with Fissile Build the BOSH release first and convert it with fissile.\nTo generate a docker image from the BOSH release, you should use the following subcommand:\nfissile build release-image For more information on how to use the command, please refer to the related documentation. For a real example, see build.sh.\nUploading The Image Depending on your cluster, you will need a way to get the locally built image into the Kubernetes registry.\nWith minikube you can build directly on minikube\u0026rsquo;s Docker. Switch to that docker daemon by running eval $(minikube docker-env), before you build the image with fissile.\nWith kind, you need to use kind load docker-image after building the image, to make it available, i.e.:\nkind load docker-image docker.io/org/nats:0.1-dev Modify Kubecf to Use the New Image Add an operations file to Kubernetes with the new image location. The example below uses NATS as the example for a BOSH release.\nkubectlapply-f-\u0026lt;\u0026lt;EOF---apiVersion:v1kind:ConfigMapmetadata:name:nats-devdata:ops:|-type:replacepath:/releases/name=nats?value:name:natsurl:docker.io/org/natsversion:0.1-devsha1:~EOF Then, when running helm install kubecf, refer to that image:\nhelm install ... --set \u0026#39;operations.custom={nats-dev}\u0026#39; Note: You can also unpack the helm release and modify it directly. There is no need to zip the release again, as helm install kubecf/ is able to install the unpacked release.\nNote further that the above is an example of how to use the first kind of customization feature noted in the main README.\nIntegrating the Release in Kubecf With Quarks and Kubecf, BOSH releases can largely be used just the same as with a BOSH director. There are a few things Quarks offers, however, to make the adaptation to the Kubernetes environment easier.\nBPM BPM configurations for jobs are parsed from a rendered bpm.yml, as usual. But if need be, it is also possible to override the BPM configuration in the deployment manifest in the quarks field. See the bpm documentation for details on how to configure BPM.\nExample:\ninstance_groups:-name:natsinstances:2jobs:-name:natsproperties:quarks:bpm:processes:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Note: The next section on ops files explains how this can be applied without the need to modify the original deployment manifest using ops files.\nOperation Files ops files can be used to modify arbitrary parts of the deployment manifest before it is applied. To do so, create a file in the directory deploy/helm/scf/assets/operations/instance_groups and it will be automagically applied during installation, courtesy of the bazel machinery.\nThe ops file for the example above could look like this:\n-type:replacepath:/instance_groups/name=nats/jobs/name=nats/properties/quarks?/bpm/processesvalue:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\u0026#34;/var/vcap/jobs/nats/config/nats.conf\u0026#34; Testing With Kubecf After upload and integration, it is possible to build and deploy Kubecf according to any of the recipes listed by the main README.\n","excerpt":"Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The …","ref":"https://kubecf.suse.dev/docs/tutorials/bosh-integration/","title":"Bosh releases integration"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf locally using:\n Minikube to manage a local Kubernetes cluster. A cf-operator pinned with Bazel. Kubecf built and deployed from the sources in the current checkout.  Minikube Minikube is one of several projects enabling the deployment, management and tear-down of a local Kubernetes cluster.\nThe Kubecf Bazel workspace contains targets to deploy and/or tear-down a Minikube-based cluster. Using these has the advantage of using a specific version of Minikube. On the other side, the reduced variability of the development environment is a disadvantage as well, possibly allowing portability issues to slide through.\n   Operation Command     Deployment bazel run //dev/minikube:start   Tear-down bazel run //dev/minikube:delete    Attention, Dangers Minikube edits the Kubernetes configuration file referenced by the environment variable KUBECONFIG, or ~/.kube/config.\nTo preserve the original configuration either make a backup of the relevant file, or change KUBECONFIG to a different path specific to the intended deployment.\nAdvanced configuration The local Minikube Documentation explains the various environment variables which can be used to configure the resources used by the cluster (CPUs, memory, disk size, etc.) in detail.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same kube cluster Kubecf will be deployed to.\nDeployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down cf-operator:\n   Operation Command     Deployment bazel run //dev/cf_operator:apply   Tear-down bazel run //dev/cf_operator:delete    Kubecf With all the prequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nSystem domain The main configuration to set for kubecf is its system domain. For the Minikube foundation we have to specify it as:\necho \u0026#34;system_domain: $(minikube ip).xip.io\u0026#34; \\  \u0026gt; \u0026#34;$(bazel info workspace)/dev/kubecf/system_domain_values.yaml\u0026#34; Deployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down kubecf from the sources:\n   Operation Command     Deployment bazel run //dev/kubecf:apply   Tear-down bazel run //dev/kubecf:delete    In this default deployment kubecf is launched without Ingress, and uses the Diego scheduler.\nAccess Accessing the cluster from outside of the minikube VM requires ingress to be set up correctly.\nTo access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.$(minikube ip).xip.io\u0026#34; # Copy the admin cluster password. acp=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${acp}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, add a file matching the pattern *values.yaml to the directory dev/kubecf and containing\nfeatures:eirini:enabled:true before deploying kubecf.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/kubecf-scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/kubecf-tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; \\  --set \u0026#34;controller.service.externalIPs={$(minikube ip)}\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nThe last flag in the command above assigns the external IP of the cluster to the Ingress Controller service.\nConfigure kubecf Place a file matching the pattern *values.yaml into the directory dev/kubecf and containing\nfeatures:ingress:enabled:true","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/tutorials/deploy-minikube/","title":"Deploy KubeCF in Minikube"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/concepts/","title":"Concepts"},{"body":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator enables the deployment of BOSH Releases, especially Cloud Foundry, to Kubernetes.\nIt\u0026rsquo;s implemented as a k8s operator, an active controller component which acts upon custom k8s resources.\n Installation notes Incubation Proposal: Containerizing Cloud Foundry Slack: #quarks-dev on https://slack.cloudfoundry.org Backlog: Pivotal Tracker Docker: https://hub.docker.com/r/cfcontainerization/cf-operator/tags  ","excerpt":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator enables the deployment of BOSH …","ref":"https://kubecf.suse.dev/docs/concepts/operator/","title":"Cloud Foundry Operator"},{"body":"Project Quarks is an incubating effort within the Cloud Foundry Foundation that packages Cloud Foundry Application Runtime as containers instead of virtual machines, enabling easy deployment to Kubernetes.\nThe resulting containerized CFAR provides an identical developer experience to that of BOSH-managed Cloud Foundry installations, requires less infrastructure capacity and delivers an operational experience that is familiar to Kubernetes operators.\n","excerpt":"Project Quarks is an incubating effort within the Cloud Foundry Foundation that packages Cloud …","ref":"https://kubecf.suse.dev/docs/concepts/quarks/","title":"Project Quarks"},{"body":" Once you have deployed KubeCF, you might want to validate it by running Cloud Foundry\u0026rsquo;s smoke tests. For that purpose, KubeCF\u0026rsquo;s helm chart ships the Cloud Foundry smoke tests, packaged inside of a deployment\u0026rsquo;s instance group.\nTriggering the smoke tests The smoke tests are run by a CF-Operator qjob (a wrapper on Kube jobs). That job is defined to not trigger automatically by default. To start it, patch the job with trigger strategy \u0026ldquo;now\u0026rdquo;:\n$ kubectl get qjob --namespace scf --output name 2\u0026gt; /dev/null | grep smoke-tests quarksjob.quarks.cloudfoundry.org/smoke-tests $ kubectl patch quarksjob.quarks.cloudfoundry.org/smoke-tests \\ --namespace kubecf --type merge --patch \\ '{ \u0026quot;spec\u0026quot;: { \u0026quot;trigger\u0026quot;: { \u0026quot;strategy\u0026quot;: \u0026quot;now\u0026quot; } } }'  This will start the job, which creates a smoke-tests-\u0026lt;id\u0026gt; pod. Inside that pod, there\u0026rsquo;s a container called smoke-tests-smoke-tests with the test run.\nYou can, as usual, see the resulting logs from the smoke-tests pod with:\n$ kubectl logs -f smoke-tests-614496c133797980-bm2g4 --namespace scf \\ --container smoke-tests-smoke-tests Running smoke tests... Running binaries smoke/isolation_segments/isolation_segments.test smoke/logging/logging.test smoke/runtime/runtime.test [1592406628] CF-Isolation-Segment-Smoke-Tests - 4 specs - 7 nodes SSSS SUCCESS! 13.717155722s [1592406628] CF-Logging-Smoke-Tests - 2 specs - 7 nodes S• SUCCESS! 36.291956367s [1592406628] CF-Runtime-Smoke-Tests - 2 specs - 7 nodes S• SUCCESS! 30.456607562s Ginkgo ran 3 suites in 1m21.517660359s Test Suite Passed  The pod will exit with a return code of 0 if successful, and other if not.\n","excerpt":"Once you have deployed KubeCF, you might want to validate it by running Cloud Foundry\u0026rsquo;s smoke …","ref":"https://kubecf.suse.dev/docs/tutorials/run-smoke-tests/","title":"Run smoke tests"},{"body":" Upgrading from a previous deployment Upgrading is roughly the same as doing an initial deployment; however, please use helm upgrade intead of helm install.\nIf you are providing a configuration file that was originally from a previous deployment, please take care to review the configuration to ensure you are not maintaining default values from a previous version unintentionally. Where possible, not specifying configuration that maintains the default values will prevent accidentally changing them if the defaults have changed in the new version.\nUpgrading from SCF Please refer to the SUSE documentation. Exporting data from the SCF insallation and importing into KubeCF is required.\n","excerpt":"Upgrading from a previous deployment Upgrading is roughly the same as doing an initial deployment; …","ref":"https://kubecf.suse.dev/docs/tasks/upgrade/","title":"Upgrading KubeCF deployments"},{"body":" Deploying on RHEL / CentOS 7 -based Kubernetes If you are deploying with diego (that is, Eirini is not enabled) on top of a RHEL / CentOS 7 based cluster, please make sure that the user.max_user_namespaces sysctl is set to a large number. Do this on any worker node that may host any diego-cell workloads:\nsudo sh -c \u0026#39;sysctl -w user.max_user_namespaces=15076 | tee -a /etc/sysctl.conf\u0026#39; This is not necessary on RHEL / CentOS 8 based systems.\n","excerpt":"Deploying on RHEL / CentOS 7 -based Kubernetes If you are deploying with diego (that is, Eirini is …","ref":"https://kubecf.suse.dev/docs/tasks/deploy/","title":"Deploy KubeCF"},{"body":" Rotating secrets is in general the process of updating one or more secrets to new values and restarting all affected pods so that they will use these new values.\nMost of the process is automatic. How to trigger it is explained in the following document.\nBeyond this, the keys used to encrypt the Cloud Controller Database (CCDB) can also be rotated, however, they do not exist as general secrets of the KubeCF deployment. This means that the general process explained above does not apply to them.\nThe audience of this document are:\n Developers working on KubeCF.\n Operators deploying KubeCF.\n  Background One of the features KubeCF (or rather the cf-operator it sits on top of) provides is the ability to declare secrets (passwords and certificates) and have the system automatically generate something suitably random for such on deployment, and distribute the results to the pods using them.\nThis removes the burden from human operators to come up with lots of such just to have all the internal components of KubeCF properly wired up for secure communication.\nHowever, even with this, operators may wish to change such secrets from time to time, or on a schedule. In other words, re-randomize the board, and limit the lifetime of any particular secret.\nAs a note on terminology, this kind of change is called rotating a secret.\nThis document describes how this can be done, in the context of KubeCF.\nFinding secrets Retrieve the list of all secrets maintained by a KubeCF deployment via\nkubectl get quarkssecret --namespace kubecf  To see the information about a specific secret, for example the NATS password, use\nkubectl get quarkssecret --namespace kubecf kubecf.var-nats-password --output yaml  Note that each quarkssecret has a corresponding regulare k8s secret it controls.\nkubectl get secret --namespace kubecf kubectl get secret --namespace kubecf kubecf.var-nats-password --output yaml  Requesting a rotation for a specific secret We keep using kubecf.var-nats-password as our example secret.\nTo rotate this secret:\n Create a YAML file for a ConfigMap of the form:\n --- apiVersion: v1 kind: ConfigMap metadata: name: rotate-kubecf.var-nats-password labels: quarks.cloudfoundry.org/secret-rotation: \u0026quot;true\u0026quot; data: secrets: '[\u0026quot;kubecf.var-nats-password\u0026quot;]'  Note, while the name of this ConfigMap can be technically anything (allowed by k8s syntax) we recommend using a name derived from the name of the secret itself, to make the connection clear.\nNote further that while this example rotates only a single secret, the data.secrets key accepts an array of secret names, allowing the simultaneous rotation of many secrets together.\n Apply this ConfigMap using:\n kubectl apply --namespace kubecf -f /path/to/your/yaml/file  The cf-operator will process this ConfigMap due the label\n quarks.cloudfoundry.org/secret-rotation: \u0026quot;true\u0026quot;  and knows that it has to invoke a rotation of the referenced secrets.\nThe actions of the cf-operator can be followed in its log.\n After the cf-operator has done the rotation, i.e. has not only changed the secrets, but also restarted all affected pods (the users of the rotated secrets), delete the trigger config map again:\nkubectl delete \u0026ndash;namespace kubecf -f /path/to/your/yaml/file\n   Rotating the CCDB encryption keys IMPORTANT - Always backup the database before rotating the encryption key.\nThe key used to encrypt the database is generated the first time kubecf is deployed. It is based on the Helm values:\nccdb:encryption:rotation:key_labels:-encryption_key_0current_key_label:encryption_key_0 For each label under key_labels, kubecf will generate an encryption key. The current_key_label indicates which key is currently being used.\nIn order to rotate the CCDB encryption key, add a new label to key_labels (keeping the old labels), and mark the current_key_label with the newly added label. Example:\nccdb:encryption:rotation:key_labels:-encryption_key_0-encryption_key_1current_key_label:encryption_key_1 IMPORTANT - key labels should be less than 240 characters long.\nThen, update the kubecf Helm installation. After Helm finishes its updates, trigger the rotate-cc-database-key errand:\nNote - the following command assumes the Helm installation is named kubecf and it was installed to the kubecf namespace. These values may be different depending on how kubecf was installed.\nkubectl patch qjob kubecf-rotate-cc-database-key \\  --namespace kubecf \\  --type merge \\  --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;trigger\u0026#34;:{\u0026#34;strategy\u0026#34;:\u0026#34;now\u0026#34;}}}\u0026#39;","excerpt":"Rotating secrets is in general the process of updating one or more secrets to new values and …","ref":"https://kubecf.suse.dev/docs/tasks/secrets/","title":"Secret rotation KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/","title":"Core Tasks"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tasks/bosh/","title":"Test BOSH releases"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/docs/tutorials/","title":"Tutorials"},{"body":"To see the docs referring to internals or specific code areas of KubeCF, we recommend looking at: https://github.com/cloudfoundry-incubator/kubecf/tree/master/doc\nA good starting point is the doc\u0026rsquo;s Contribution guide, and its directory organization table.\n","excerpt":"To see the docs referring to internals or specific code areas of KubeCF, we recommend looking at: …","ref":"https://kubecf.suse.dev/docs/reference/","title":"Reference"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nIt provides a basic overview of various aspects of the project below, and uses these overviews as the launching points to other documents which go deeper into the details of each aspect.\n Table of Contents (Aspects)  Deployment Pull Requests Source Organization Updating Subcharts Docker Images Linting Patching [BOSH Development Workflow] Rotating secrets  Deployment Kubecf is built on top of a number of technologies, namely Kubernetes, Helm (charts), and the cf-operator.\nFor all these we have multiple choices for installing them, and various interactions between the choices influence the details of the commands to use.\nInstead of trying to document all the possibilities and all their interactions at once, supporting documents will describe specific combinations of choices in detail, from the bottom up.\n   Document Description     Local Minikube Minikube + Operator + Kubecf   General Kube Any Kube + Operator/Helm + Kubecf/Helm    Pull Requests The general work flow for pull requests contributing bug fixes, features, etc. is:\n Branch or Fork the suse/kubecf repository, depending on permissions.\n Implement the bug fix, feature, etc. on that branch/fork.\n Submit a pull request based on the branch/fork through the github web interface, against the master branch.\n Developers will review the content of the pull request, asking questions, requesting changes, and generally discussing the submission with the submitter and among themselves.\n PRs from branches of this repository are automatically tested in the CI. For forks, you should ask a maintainer of this repository to trigger a build. Automated triggers have been disabled for security reasons.\n After all issues with the request are resolved, and CI has passed, a developer will merge it into master.\n Note that it may be necessary to rebase the branch/fork to resolve any conflicts due to other PRs getting merging while the PR is under discussion.\nSuch a rebase will be a change request from the developers to the contributor, on the assumption that the contributor is best suited to resolving the conflicts.\n  Source Organization The important directories of the kubecf sources, and their contents are shown in the table below. Each directory entry links to the associated documentation, if we have any.\n   Directory Content     top Documentation entrypoint, License,    Main workspace definitions.   top/\u0026hellip;/README.md Directory-specific local documentation.   top/bosh/releases Support for runtime patches of a kubecf deployment.   top/doc Global documentation.   top/dev/cf_deployment/bump Tools to support updating the cf deployment    manifest used by kubecf.   top/dev/cf_cli Deploy cf cli into a helper pod from which to then    inspect the deployed Kubecf   top/dev/kube Tools to inspect kube clusters and kubecf deployments.   top/dev/kubecf Kubecf chart configuration   top/deploy/helm/kubecf Templates and assets wrapping a CF deployment    manifest into a helm chart.   top/rules Supporting scripts.   top/testing Scripts with specific testing   top/scripts Developer scripts used by make to start a k8s cluster    (for example on kind), lint, build, run \u0026amp; test kubecf   top/scripts/tools Developer scripts pinning the development dependencies    Updating subcharts The kubecf helm chart includes a number of subcharts. They are declared in requirements.yaml. For the convenience of development they are included in unpacked form directly in this repo, so version changes can be inspected with regular git tools, and the subcharts can be searched with grep etc.\nThe procedure to update the version of a subchart is:\nvi deploy/helm/kubecf/requirements.yaml ./dev/helm/update_subcharts.sh git commit  Docker Images The docker images used by kubecf to run jobs in container use a moderately complex naming scheme.\nThis scheme is explained in a separate document: The Naming Of Docker Images in kubecf.\nLinting Currently, 3 linters are available: shellcheck, yamllint, \u0026amp; helm linting.\nInvoke these linters with\nmake lint to run shellcheck on all .sh files found in the entire checkout, or yamllint on all .yaml or .yml files respectively, and report any issues found. The last option runs helm lint (without --strict) on the generated helm chart.\nSee the authoritative list of linters being called in the make lint target.\nPatching Background The main goal of the CF operator is to take a BOSH deployment manifest, deploy it, and have it run as-is.\nNaturally, in practice, this goal is not quite reached yet, requiring patching of the deployment manifest in question, and/or the involved releases, at various points of the deployment process. The reason behind a patch is generally fixing a problem, whether it be from the translation into the kube environment, an issue with an underlying component, or something else.\nThen, there are features, given the user of the helm chart wrapped around the deployment manifest the ability to easily toggle various preset configurations, for example the use of eirini instead of diego as the application scheduler.\nFeatures A feature of kubecf is usually implemented using a combination of Helm templating and BOSH ops files.\nThe helm templating is used to translate the properties in the chart\u0026rsquo;s values.yaml to the actual actions to take, by including/excluding chart elements, often the BOSH ops files containing the structured patches modifying the deployment itself (changing properties, adding/removing releases, (de)activating jobs, etc.)\nThe helm templating is applied when the kubecf chart is deployed.\nThe ops files are then applied by the operator, transforming the base manifest from the chart into the final manifest to deploy.\nCustomization Kubecf provides two mechanisms for customization during development (and maybe by operators ?):\n The property .Values.operations.custom of the chart is a list of names for kube configmaps containing the texts of the ops files to apply beyond the ops files from the chart itself.\nNote that we are talking here about a yaml structure whose data.ops property is a text block holding the yaml structure of an ops file.\nThere is no tooling to help the writer with the ensuing quoting hell.\nNote further that the resulting config maps have to be applied, i.e. uploaded into the kube cluster before deploying the kubecf helm chart with its modified values.yaml.\nFor example, kubectl apply the object below\n ```yaml --- apiVersion: v1 kind: ConfigMap metadata: name: configmap_name data: ops: |- some_random_ops ```  and then use\n ```yaml operations: custom: - configmap_name ```  in the values.yaml (or an equivalent --set option) as part of a kubecf deployment to include that ops file in the deployment.\nThe [BOSH Development Workflow] is an example of its use.\n[BOSH Development Workflow]: bosh-release-development.md\n The second mechanism allows the specification of any custom BOSH property for any instancegroup and job therein.\nJust specifying\n ```yaml properties: instance-group-name: job-name: some-property: some-value ```  in the values.yaml for the kubecf chart causes the chart to generate and use an ops file which applies the assignment of some-value to some-property to the specified instance group and job during deployment.\nAn example of its use in Kubecf is limiting the set of test suites executed by the CF acceptance tests.\n  Both forms of customization assume a great deal of familiarity on the part of the developer and/or operator with the BOSH releases, instance groups and jobs underlying the CF deployment manifest, i.e. which properties exist, what changes to them mean and how they affect the system.\nPatches In SCF v2, the predecessor to kubecf, the patches scripts enabled developers and maintainers to apply general patches to the sources of a job (i.e. configuration templates, script sources, etc.) before that job was rendered and then executed. At the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nPre render scripts are the equivalent feature of the CF operator.\nKubecf makes use of this feature to fix a number of issues in the deployment. The relevant patch scripts are found under the directory bosh/releases/pre_render_scripts.\nWhen following the directory structure explained by the README, the bazel machinery for generating the kubecf helm chart will automatically convert these scripts into the proper ops files for use by the CF operator.\nAttention All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\nRotating Secrets Rotating secrets is in general the process of updating one or more secrets to new values and restarting all affected pods so that they will use these new values.\nMost of the process is automatic. How to trigger it is explained in Secret Rotation.\nBeyond this, the keys used to encrypt the Cloud Controller Database (CCDB) can also be rotated, however, they do not exist as general secrets of the KubeCF deployment. This means that the general process explained above does not apply to them.\nTheir custom process is explained in CCDB encryption key rotation.\n","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":" KubeCF is under active development. Note there might be discrepancies between the docs and latest releases.\n ","excerpt":" KubeCF is under active development. Note there might be discrepancies between the docs and latest …","ref":"https://kubecf.suse.dev/docs/","title":"Documentation"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf using:\n A generic kubernetes cluster. A released cf-operator helm chart. A released kubecf helm chart.  Kubernetes In contrast to other recipes, we are not set on using a local cluster. Any Kubernetes cluster will do, assuming that the following requirements are met:\n Presence of a default storage class (provisioner).\n For use with a diego-based kubecf (default), a node OS with XFS support.\n For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   This can be any of, but not restricted to:\n GKE (Notes) AKS EKS  Note that how to deploy and tear-down such a cluster is outside of the scope of this recipe.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same Kubernetes cluster that Kubecf will be deployed to.\nHere we are not using development-specific dependencies like bazel, but only generic tools, i.e. kubectl and helm.\nInstalling and configuring Helm is the same regardless of the chosen foundation, and assuming that the cluster does not come with Helm Tiller pre-installed.\nDeployment and Tear-down helm install cf-operator \\  --namespace cfo \\  --set \u0026#34;global.singleNamespace.name=kubecf\u0026#34; \\  https://cf-operators.s3.amazonaws.com/helm-charts/cf-operator-5.0.0%2B0.gd7ac12bc.tgz In the example above, version 5.0.0 of the operator was used. Look into the cf_operator section of the top-level dependencies.yaml file to find the version of the operator validated against the current kubecf master.\nNote: \u0026gt; The above helm install will generate many controllers spread over multiple pods inside the cfo namespace. \u0026gt; Most of these controllers run inside the cf-operator pod. \u0026gt; \u0026gt; The global.singleNamespace.name=kubecf path tells the controllers to watch for CRD´s instances into the kubecf namespace. \u0026gt; \u0026gt; The cf-operator helm chart will generate the kubecf namespace during installation, and eventually one of the controllers will use a webhook to label this namespace with the cf-operator-ns key. \u0026gt; \u0026gt; If the kubecf namespace is deleted, but the operators are still running, they will no longer know which namespace to watch. This can lead to problems, so make sure you also delete the pods inside the cfo namespace, after deleting the kubecf namespace.\nNote how the namespace the operator is installed into (cfo) differs from the namespace the operator is watching for deployments (kubecf).\nThis form of deployment enables restarting the operator because it is not affected by webhooks. It further enables the deletion of the Kubecf deployment namespace to start from scratch, without redeploying the operator itself.\nTear-down is done with a standard helm delete ... command.\nKubecf With all the prerequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nThis again uses helm and a released helm chart.\nDeployment and Tear-down helm install kubecf \\  --namespace kubecf \\  https://kubecf.s3.amazonaws.com/kubecf-v2.3.0.tgz \\  --set \u0026#34;system_domain=kubecf.suse.dev\u0026#34; In this default deployment, kubecf is launched without Ingress, and it uses the Diego scheduler.\nTear-down is done with a standard helm delete ... command.\nAccess To access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \u0026#34;https://api.\u0026lt;domain\u0026gt;\u0026#34; # Copy the admin cluster password. admin_pass=$(kubectl get secret \\  --namespace kubecf var-cf-admin-password \\  -o jsonpath=\u0026#39;{.data.password}\u0026#39; \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \u0026#34;${admin_pass}\u0026#34; Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, use the option --set features.eirini.enabled=true when deploying kubecf from its chart.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \u0026#34;tcp.2222=kubecf/scheduler:2222\u0026#34; \\  --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.start\u0026gt;=kubecf/tcp-router:\u0026lt;services.tcp-router.port_range.start\u0026gt;\u0026#34; \\  ... --set \u0026#34;tcp.\u0026lt;services.tcp-router.port_range.end\u0026gt;=kubecf/tcp-router:\u0026lt;services.tcp-router.port_range.end\u0026gt;\u0026#34; The tcp.\u0026lt;port\u0026gt; option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nConfigure kubecf Use the Helm option --set features.ingress.enabled=true when deploying kubecf.\nExternal Database By default, kubecf includes a single-availability database provided by the cf-mysql-release. Kubecf also exposes a way to use an external database via the Helm property features.external_database. Check the values.yaml for more details.\nFor local development with an external database, the command bash ./scripts/deploy_mysql.sh will bring a mysql database up and running ready to be consumed by kubecf.\nAn example for the additional values to be provided to make kubecf:apply:\nfeatures:external_database:enabled:truetype:mysqlhost:kubecf-mysql.kubecf-mysql.svcport:3306databases:uaa:name:uaapassword:\u0026lt;root_password\u0026gt; username: rootcc:name:cloud_controllerpassword:\u0026lt;root_password\u0026gt; username: rootbbs:name:diegopassword:\u0026lt;root_password\u0026gt; username: rootrouting_api:name:routing-apipassword:\u0026lt;root_password\u0026gt; username: rootpolicy_server:name:network_policypassword:\u0026lt;root_password\u0026gt; username: rootsilk_controller:name:network_connectivitypassword:\u0026lt;root_password\u0026gt; username: rootlocket:name:locketpassword:\u0026lt;root_password\u0026gt; username: rootcredhub:name:credhubpassword:\u0026lt;root_password\u0026gt; username: root","excerpt":"The intended audience of this document are developers wishing to contribute to the Kubecf project. …","ref":"https://kubecf.suse.dev/docs/getting-started/kubernetes-deploy/","title":"Deploy on Kubernetes"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/index.json","title":""},{"body":"  #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_bottom.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/about/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_bottom.jpg); } }  About KubeCF Cloud Foundry in Kubernetes.        KubeCF uses the Quarks operator The Quarks project extends Kubernetes to understand BOSH     KubeCF is designed to bring existing CF components in Kubernetes, leveraging existing BOSH releases.       Smooth transitioning from BOSH to Kubernetes native components      ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"https://kubecf.suse.dev/about/","title":"About KubeCF"},{"body":" KubeCF community This is the place to start your journey with KubeCF and hang out with the KubeCF community!\nThe following section will provide useful links to become a KubeCF contributor, and guide you to the first steps into KubeCF.\nCode of Conduct Important Before going ahead, please review the KubeCF Code of Conduct.\nKubeCF follows the CloudFoundry Foundation Code of Conduct. Unacceptable behavior may be reported by contacting the CloudFoundry Foundation via conduct@cloudfoundry.org.\nUseful links  Documentation Contribution Guidelines Deploy locally with kind  Not sure where to start? have a quick look at the open issues that are marked as \u0026ldquo;Good first issues\u0026rdquo;\n","excerpt":"KubeCF community This is the place to start your journey with KubeCF and hang out with the KubeCF …","ref":"https://kubecf.suse.dev/community/","title":"Community"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_2300986_1920x1080_fill_q75_catmullrom_top.jpg); } }  KubeCF Learn More   Download   Cloud Foundry on Kubernetes\n          Cloud Foundry built for Kubernetes (formerly SUSE/scf v3 branch). It makes use of the Cloud Foundry Operator, which is incubating under Project Quarks.       CFAR KubeCF produces builds of CFAR which can be deployed to Kubernetes with Helm and managed by the cf-operator.\n   BOSH KubeCF already consumes your BOSH release and packages it for Kubernetes\n   Integration cf-operator extends Kubernetes to understand BOSH\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"https://kubecf.suse.dev/","title":"KubeCF"},{"body":"","excerpt":"","ref":"https://kubecf.suse.dev/search/","title":"Search Results"}]
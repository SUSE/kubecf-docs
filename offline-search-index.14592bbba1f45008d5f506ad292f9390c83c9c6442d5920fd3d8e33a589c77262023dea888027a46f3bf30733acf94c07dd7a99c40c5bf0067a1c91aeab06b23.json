[{"body":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the Cloud Foundry Operator to deploy and track CF Deployments, consuming directly also BOSH Releases, among Kubernetes Native components.\nWhere should I go next?  Deploy KubeCF: Get started with KubeCF Tutorials: Check out some tutorials!  ","excerpt":" KubeCF is a Cloud Foundry distribution for Kubernetes.\nIt uses the …","ref":"/docs/overview/","title":"Overview"},{"body":" The make cats target starts a run of the Cloud Foundry Acceptance Tests.\nSee also the entire set of available tests.\nLimiting test suites To limit the test groups to run, set the BOSH property acceptance_tests.include as documented. To do so, adjust the properties key in values.yaml to specify the groups desired. For example:\nproperties:acceptance-tests:acceptance-tests:acceptance_tests:include:\"+docker,-ssh\" Note that this is an example of how to use the second kind of customization feature noted in the main README.\n","excerpt":" The make cats target starts a run of the Cloud Foundry Acceptance …","ref":"/docs/reference/tests/","title":"Tests"},{"body":" Prerequisites  A Kubernetes cluster Presence of a default storage class (provisioner). For use with a diego-based kubecf (default), a node OS with XFS support.  For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.  Helm 3  Installation KubeCF is packaged as an Helm chart.\nCheck the release page for a full list of the official releases.\nNightly builds can be found on the KubeCF public s3 bucket.\nThere are two assets for each release: one is the “kubecf-bundle” which contains the KubeCF chart along with the cf-operator version required to deploy it, while the standalone chart containts only the KubeCF chart.\nThe standalone chart (kubecf.tgz) contains a Metadata.yaml file which indicates the version of the cf-operator to install for a successfull deployment.\nTry it out! Detailed deployment instructions can be found on the walkthrough page.\n","excerpt":" Prerequisites  A Kubernetes cluster Presence of a default storage …","ref":"/docs/deployment/","title":"Deploying KubeCF on Kubernetes"},{"body":" Before you begin This guideline will provide a quick way to deploy KubeCF with K3S and should be used only for evaluation purposes.\nPrerequisites  K3s Helm kubectl KubeCF release  Before you begin you need a k3s cluster running and a KubeCF release copy extracted in a folder which will be used in the tutorial, you will find notes/tips to deploy on a VM based environment.\nK3s installation notes curl -sfL https://raw.githubusercontent.com/rancher/k3s/master/install.sh | INSTALL_K3S_EXEC=\"server --no-deploy traefik --node-external-ip $EXT_IP\" sh\n  --node-external-ip Replace $EXT_IP with the node external IP. For example, the IP which can be reached from over the network. --no-deploy traefik It’s to disable traefik deploy by default   Depending on your Linux Distribution, you might want to install relevant packages for k3s to work properly, remember to check Installation requirements and the appropriate section for your OS. For example in OpenSUSE Tumbleweed, zypper in -y kernel-default which curl wget open-iscsi or switch to iptable-legacy for Debian.\n agent token to join new nodes in /var/lib/rancher/k3s/server/node-token joining a node in the k3s cluster: curl -sfL https://raw.githubusercontent.com/rancher/k3s/master/install.sh | INSTALL_K3S_EXEC=\"agent --node-external-ip ext-ip\" K3S_URL=https://externalip:6443 K3S_TOKEN=agent_token sh - Take the kubeconfig from /etc/rancher/k3s/k3s.yaml on the master node and edit server with the node ip reachable from outside Debian hosts might need specific kernels, e.g. apt install linux-image-4.19-cloud-amd64  Longhorn installation notes Make sure to have installed host required dependencies (iscsi) ( for e.g. in Tumbleweed: zypper in -y kernel-default which curl wget open-iscsi ) and enable them on boot systemctl enable --now iscsid.\n$ git clone https://github.com/longhorn/longhorn # Tweak depending on your number of nodes $ cat \u003c\u003cEOF \u003e\u003elonghorn_values.yaml persistence: defaultClass: true defaultClassReplicaCount: 1 EOF $ kubectl create namespace longhorn-system $ helm install longhorn ./longhorn/chart/ --namespace longhorn-system --values longhorn_values.yaml SUSE charts Add the SUSE helm repository, where we will install nginx-ingress from:\n$ helm repo add suse https://kubernetes-charts.suse.com/ \"suse\" has been added to your repositories $ helm repo update Deploy Nginx-ingress We will use the nginx-ingress with KubeCF in the following example.\nLet’s create a nginx_ingress.yaml values file with the following content:\n$ cat \u003c\u003cEOF \u003e\u003enginx_ingress.yaml tcp: 2222: \"kubecf/scheduler:2222\" 20000: \"kubecf/tcp-router:20000\" 20001: \"kubecf/tcp-router:20001\" 20002: \"kubecf/tcp-router:20002\" 20003: \"kubecf/tcp-router:20003\" 20004: \"kubecf/tcp-router:20004\" 20005: \"kubecf/tcp-router:20005\" 20006: \"kubecf/tcp-router:20006\" 20007: \"kubecf/tcp-router:20007\" 20008: \"kubecf/tcp-router:20008\" EOF we will use it to install the Ingress and support tcp routing in CF.\nExternalIPs  If you want to expose the ingress over specific externalIPs in your cluster, you can while installing the ingress with helm\nwe will find th node external IPs which we are interested in:\n$ kubectl get node NAME STATUS ROLES EXTERNAL-IP host-1 Ready master 203.0.113.1 host-2 Ready node 203.0.113.2 host-3 Ready node 203.0.113.3 and update the nginx_ingress values file:\n$ cat \u003c\u003cEOF \u003e\u003enginx_ingress.yaml service: externalIPs: - 203.0.113.2 - 203.0.113.3 EOF  Let’s install nginx-ingress with helm:\n$ kubectl create namespace nginx-ingress namespace/nginx-ingress created $ helm install nginx-ingress suse/nginx-ingress \\ --namespace nginx-ingress \\ --values nginx_ingress.yaml Deploy Quarks-Operator $ kubectl create namespace cf-operator namespace/cf-operator created $ helm install cf-operator ./cf-operator.tgz --namespace cf-operator --set \"global.singleNamespace.name=kubecf\" Deploy KubeCF We will use a nip.io domain, for whose those aren’t familiar, it’s a free redirection service. Meaning that we can assign to kubecf an entire *.domain like myip.nip.io to make our deployment reachable from outside. If you aren’t interested in the ingress option, just skip the instructions in the features block:\ncat \u003c\u003cEOF \u003e\u003e kubecf-config-values.yaml system_domain: $EXT_IP.nip.io credentials: cf_admin_password: testcluster uaa_admin_client_secret: testcluster features: ingress: enabled: true tls: crt: | -----BEGIN CERTIFICATE----- MIIE8jCCAtqgAwIBAgIUT/Yu/Sv8AUl5zHXXEKCy5RKJqmYwDQYJKoZIhvcMOQMM [...] xC8x/+zB7XlvcRJRio6kk670+25ABP== -----END CERTIFICATE----- key: | -----BEGIN RSA PRIVATE KEY----- MIIE8jCCAtqgAwIBAgIUSI02lj2b2ImLy/zMrjNgW5d8EygwQSVJKoZIhvcYEGAW [...] to2WV7rPMb9W9fd2vVUXKKHTc+PiNg== -----END RSA PRIVATE KEY----- EOF if you don’t have a certificate, and if you are setting up a staging environment, you can generate one with quarks-secret and avoid to provide it in the values, for e.g. by applying (mind to replace $EXT_IP with your ip ):\napiVersion:quarks.cloudfoundry.org/v1alpha1kind:QuarksSecretmetadata:name:nip.quarks.canamespace:kubecfspec:request:certificate:alternativeNames:nullcommonName:$EXT_IP.nip.ioisCA:truesignerType:localsecretName:nip.secret.catype:certificate---apiVersion:quarks.cloudfoundry.org/v1alpha1kind:QuarksSecretmetadata:name:nip.quarks.tlsnamespace:kubecfspec:request:certificate:CAKeyRef:key:private_keyname:nip.secret.caCARef:key:certificatename:nip.secret.caalternativeNames:-\"*.$EXT_IP.nip.io\"commonName:kubeTlsTypeCertisCA:falsesignerType:localsecretName:kubecf-ingress-tlstype:tls or by providing your own certificate in kubecf/kubecf-ingress-tls a tls certificate type.\nStorage Class  Note if you have a custom Storage class, set the relevant section in the KubeCF config values:\n $ cat \u003c\u003cEOF \u003e\u003e kubecf-config-values.yaml kube: storage_class: longhorn EOF\n Finally let’s install KubeCF:\n$ helm install kubecf --namespace kubecf ./kubecf_release.tgz --values kubecf-config-values.yaml NAME: kubecf LAST DEPLOYED: Thu Oct 1 11:34:06 2020 NAMESPACE: kubecf STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Welcome to your new deployment of KubeCF. The endpoint for use by the `cf` client is https://api.$EXT_IP.nip.io To target this endpoint and login, run cf login --skip-ssl-validation -a https://api.$EXT_IP.nip.io -u admin Please remember, it may take some time for everything to come online. You can use kubectl get pods --namespace kubecf to spot-check if everything is up and running, or watch -c 'kubectl get pods --namespace kubecf' to monitor continuously. You will be running 1 diego cell(s), with 40960Mi of disk each. The default app quota is set to 1024Mi, which means you'll have enough disk space to run about 40 apps. The online documentation (release notes, deployment guide) can be found at https://kubecf.io/docs Now you are ready to deploy Stratos\nNote on eirini After deployment if Eirini is enabled, it’s necessary to trust the CA used by eirini to pull images from internal registry on the node.\nOn each node, you can do with (needs yq on the nodes):\n$ k3s kubectl get secret bits-service-ssl -n kubecf -o yaml | yq r - 'data.ca' | base64 -d \u003e eirini-ca.crt $ cp -rfv eirini-ca.crt /etc/ssl/certs/ \u0026\u0026 systemctl restart k3s\n ","excerpt":" Before you begin This guideline will provide a quick way to deploy …","ref":"/docs/tutorials/deploy-k3s/","title":"Deploy KubeCF on K3s"},{"body":" Before you begin This guideline will provide a quick way to deploy KubeCF with Kind and should be used only for evaluation purposes.\nHere is a list of all the tools and versions used when creating these instructions:\n\u003e kind version kind v0.7.0 go1.13.6 darwin/amd64  \u003e helm version version.BuildInfo{Version:\"v3.1.1\", GitCommit:\"afe70585407b420d0097d07b21c47dc511525ac8\", GitTreeState:\"clean\", GoVersion:\"go1.13.8\"}  \u003e docker version Client: Docker Engine - Community Version: 19.03.5 API version: 1.40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07:22:34 2019 OS/Arch: darwin/amd64 Experimental: false  \u003e kubectl version Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-13T18:08:14Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"darwin/amd64\"}  \u003e cf version cf version 6.46.1+4934877ec.2019-08-23z  Installing Kind To install Kind please follow the official instructions here.\nCreate a Kind cluster Run the following commands:\n\u003e export KUBECONFIG=kubeconfig-kubecf \u003e kind create cluster --name kubecf \u003e kubectl cluster-info --context kind-kubecf  Download cf-operator and KubeCF Helm charts Download the latest release bundle (kubecf-bundle-vX.X.X.tgz) to a local directory and extract it:\n\u003e curl -s https://api.github.com/repos/cloudfoundry-incubator/kubecf/releases/latest \\ | grep -oP '\"browser_download_url\": \"\\K(.*)kubecf-bundle(.*)(?=\")' \\ | wget -qi - \u003e tar xf kubecf-bundle-v*.tgz  The archive contains cf-operator and KubeCF Helm charts (cf-operator.tgz and kubecf_release.tgz respectively).\nAll releases can be found on KubeCF releases page.\nInstalling cf-operator Before we can deploy the cf-operator we need to create the namespace:\n\u003e kubectl create ns cfo  and after we can install by running the helm command:\n\u003e helm install cf-operator \\ --namespace cfo \\ --set \"global.singleNamespace.name=kubecf\" \\ ./cf-operator.tgz  Notes:\n The singleNamespace property is set to watch the kubecf namespace for changes cf-operator version may differ between KubeCF versions  Check if the pods are up and running before moving to the next section:\n\u003e kubectl get pods -n cfo  Installing KubeCF First let’s get Kind node IP address:\nnode_ip=$(kubectl get node kubecf-control-plane \\ --output jsonpath='{ .status.addresses[?(@.type == \"InternalIP\")].address }')  and then set the properties correctly:\ncat \u003c\u003c _EOF_ \u003e values.yaml system_domain: ${node_ip}.nip.io services: router: externalIPs: - ${node_ip} kube: service_cluster_ip_range: 0.0.0.0/0 pod_cluster_ip_range: 0.0.0.0/0 _EOF_  On this example, we will use Diego instead Eirini but you can easily switch by adding the following lines into your values.yaml file:\n... features: eirini: enabled: true ...  and, you need to trust the kubernetes root CA on the kind docker container:\n\u003e docker exec -it \"kubecf-control-plane\" bash -c 'cp /etc/kubernetes/pki/ca.crt /etc/ssl/certs/ \u0026\u0026 \\ update-ca-certificates \u0026\u0026 \\ (systemctl list-units | grep containerd \u003e /dev/null \u0026\u0026 systemctl restart containerd)'  the values.yaml file should be similiar to the snippet:\nsystem_domain: 172.17.0.3.nip.io services: router: loadBalancerIP: - 172.17.0.3 kube: service_cluster_ip_range: 0.0.0.0/0 pod_cluster_ip_range: 0.0.0.0/0  Now is time to install KubeCF by running the helm command:\n\u003e helm install kubecf \\ --namespace kubecf \\ --values values.yaml \\ ./kubecf_release.tgz  Notes:\n the namespace property value is the same as cf-operator singleNamespace one  Be aware that it takes a couple of minutes to see the pods showing up on the kubecf namespace and the installation process may take 20-25 minutes depending on your internet connection speed.\nRun the following command to watch the pods progress:\n\u003e watch kubectl get pods -n kubecf  After all the pods are running you can check by running the cf cli command:\n\u003e cf api --skip-ssl-validation api.172.17.0.3.nip.io  get the admin password:\n\u003e admin_pass=$(kubectl get secret --namespace kubecf \\ var-cf-admin-password \\ -o jsonpath='{.data.password}' | base64 --decode)  and login with: cf auth admin \"${admin_pass}\"\nWhat’s next After the deployment finishes with success it’s time to give it a try by pushing an app using the cf-push cli command.\nCleaning up \u003e kind delete cluster --name kubecf  ","excerpt":" Before you begin This guideline will provide a quick way to deploy …","ref":"/docs/tutorials/deploy-kind/","title":"Deploy KubeCF on Kind"},{"body":" KubeCF allows to leverage the Kubernetes Cluster available StorageClass. When Eirini is being enabled, two additional components are deployed, the eirini-persi-extension and eirini-persi-broker.\nThe broker must be configured in order to use the storageclass to provide support for persistence data for applications pushed to CloudFoundry with Eirini.\nYou can set up default plans that are applied to the broker during deployment.\nTo configure the broker, in the KubeCF values add and adjust as needed:\neirinix:persi-broker:service-plans:-id:defaultname:\"default\"description:\"Existing default storage class\"kube_storage_class:\"default\"# Storageclass used for provisioningfree:truedefault_size:\"1Gi\"# Default size of generated PVCdefault_access_mode:\"ReadWriteOnly\"# Here you can tweak the default access mode for new PVCs Setup eirini-persi-broker with KubeCF In this section we will see how to setup the eirini-persi-broker on a KubeCF deployment.\nGet the broker password After deploying KubeCF the broker password should be automatically generated (the example assumes you have deployed KubeCF in the kubecf namespace):\n$\u003e BROKER_PASS=$(kubectl get secrets -n kubecf -o json persi-broker-auth-password | jq -r '.data.\"password\"' | base64 -d)  Create the service broker With cf-cli (you must be logged in) let’s add the broker to our cf instance:\n$\u003e cf create-service-broker eirini-persi admin $BROKER_PASS http://eirini-persi-broker:8999  Enable the service for the space Let’s enable the service (here, doing it globally):\n$\u003e cf enable-service-access eirini-persi Enabling access to all plans of service eirini-persi for all orgs as admin... OK  Check everything is ok:\n$\u003e cf service-brokers Getting service brokers as admin... name url eirini-persi http://eirini-persi-broker:8999 If eirini-persi is showed among the list of the available brokers, it means that it is configured and plans supplied during deployment are available to be consumed.\nNow the broker should be available in the marketplace, list all the broker services:\n$\u003e cf marketplace Getting services from marketplace in org system / space tmp as admin... OK service plans description eirini-persi default Eirini persistence broker  List the plans available from the broker (output might differ):\n$\u003e cf marketplace -s eirini-persi Getting service plan information for service eirini-persi as admin... OK service plan description free or paid default Eirini persistence broker free  Using the Eirini-persi broker To verify that everything works correctly it is possible to check after creating a service that the persistent volume claims are present in the Kubernetes cluster.\nCreate a service eirini-persi-broker will create PVCs associated to the services that are created:\n$\u003e cf create-service eirini-persi default eirini-persi-1 Creating service instance eirini-persi-1 in org system / space tmp as admin... OK  In this way, we can associate the service to an app, which will actually link and attach a PVC to it.\nList all the PersistentVolumeClaim and verify that a new one was created:\n$\u003e kubectl get pvc -n eirini NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE f9d4b977-0b9e-4fcb-a92e-652bc77dd7e7 Bound pvc-9960cc11-7895-11e9-afac-024267c84a6b 1Gi RWO persistent 6s  To create a service with a custom capacity, you can define the quota when creating the service.\nE.g. create a service providing a volume with 20M of quota:\n$\u003e cf create-service eirini-persi default eirini-persi-1-20M-quota -c '{\"size\": \"20M\"}'  E.g. To create a PVC which has a different access mode:\n$\u003e cf create-service eirini-persi default eirini-persi-1-rwm -c '{\"access_mode\": \"ReadWriteMany\"}'  Bind volumes to a Cloud Foundry application List the available services:\n$\u003e cf services Getting services in org system / space tmp as admin... name service plan bound apps last operation eirini-persi-1 eirini-persi default dizzylizard create succeeded  Let’s associate the service to an application (in this case dizzylizard) using eirini-persi-1:\n$\u003e cf bind-service dizzylizard eirini-persi-1 Binding service eirini-persi-1 to app dizzylizard in org system / space tmp as admin... OK  Restage our app so our change takes effect:\n$\u003e cf restage dizzylizard Restaging app dizzylizard in org system / space tmp as admin...  Access the Volume Once the service is associated, the application can access to the data of the volume mount by reading the mounted path inside the VCAP_SERVICES environment variable.\nAn example of VCAP_SERVICES generated by eirini-persi-broker:\n{\"eirini-persi\": [\t{ \"credentials\": { \"volume_id\": \"the-volume-id\" }, \"label\": \"eirini-persi\", \"name\": \"my-instance\", \"plan\": \"hostpath\", \"tags\": [ \"erini\", \"kubernetes\", \"storage\" ], \"volume_mounts\": [ { \"container_dir\": \"/var/vcap/data/de847d34-bdcc-4c5d-92b1-cf2158a15b47\", \"device_type\": \"shared\", \"mode\": \"rw\" } ] } ] } You can refer to the Cloud Foundry documentation regarding of how to access to the Volume Service, with the difference that the eirini broker will create services with the id eirini-persi.\nSee also  https://github.com/SUSE/scf/wiki/Persistence-with-Eirini-in-SCF  ","excerpt":" KubeCF allows to leverage the Kubernetes Cluster available …","ref":"/docs/tutorials/eirini_persi/","title":"Eirini Persistence broker"},{"body":" Required details  External Kubernetes IP UAA endpoint url CF admin username and password CF API url  Prepare Stratos setup config Replace \u003cEXTERNAL_IP\u003e by the external ip[s] of your cluster, adapt the port if required and store everything in a file, for example stratos.yml.\nconsole:service:externalIPs:[\"\u003cEXTERNAL_IP\u003e\"]servicePort:8443 Deploy Stratos Add the SUSE Kubernetes charts repo and deploy Stratos. Instead of the SUSE repo the Stratos chart can also be referenced from https://github.com/cloudfoundry/stratos/releases.\nhelm repo add suse https://kubernetes-charts.suse.com/ helm install --namespace stratos --name stratos --values ./stratos.yml suse/console Initial setup of Stratos As soon as Stratos is deployed connect to https://: and follow the steps.\nProvide your UAA Endpoint url, for example: https://uaa. (Make sure that there is no leading Slash at the end, otherwise UAA will ignore the authentication request.)\nThe client id is cf and the client secret stays empty.\nThe admin username is admin and password is the Cloudfoundry admin password. If you do not know the admin password yet, because for example it was autogenerated during the initial deployment of kubecf, it can be extracted with this command:\necho $(kubectl get secrets -n kubecf kubecf.var-cf-admin-password -o json | jq -r .data.password | base64 -d) The next step defines which uaa group specifies Stratos admins and does not need to be changed.\nAfter the initial setup Cloudfoundry can be added with the api endpoint url and the same cloudfoundry admin credentials.\n","excerpt":" Required details  External Kubernetes IP UAA endpoint url CF admin …","ref":"/docs/tutorials/deploy-stratos/","title":"Deploy Stratos next to KubeCF"},{"body":" Table Of Contents  Preparing the Release Image  Building a Docker Image with Fissile Uploading The Image Modify Kubecf to Use the New Image  Integrating the Release in Kubecf  BPM Operation Files  Testing With Kubecf  Preparing the Release Image BOSH release authors who want to test their development code with the Quarks operator need to build a Docker image from their release. This can be done with fissile. Afterwards, upload the image to a cluster for testing it, e.g. with Kubecf.\nBuilding a Docker Image with Fissile Build the BOSH release first and convert it with fissile.\nTo generate a docker image from the BOSH release, you should use the following subcommand:\nfissile build release-image For more information on how to use the command, please refer to the related documentation. For a real example, see build.sh.\nUploading The Image Depending on your cluster, you will need a way to get the locally built image into the Kubernetes registry.\nWith minikube you can build directly on minikube’s Docker. Switch to that docker daemon by running eval $(minikube docker-env), before you build the image with fissile.\nWith kind, you need to use kind load docker-image after building the image, to make it available, i.e.:\nkind load docker-image docker.io/org/nats:0.1-dev Modify Kubecf to Use the New Image Add an operations file to Kubernetes with the new image location. The example below uses NATS as the example for a BOSH release.\nkubectlapply-f-\u003c\u003cEOF---apiVersion:v1kind:ConfigMapmetadata:name:nats-devdata:ops:|-type:replacepath:/releases/name=nats?value:name:natsurl:docker.io/org/natsversion:0.1-devsha1:~EOF Then, when running helm install kubecf, refer to that image:\nhelm install ... --set 'operations.custom={nats-dev}' Note: You can also unpack the helm release and modify it directly. There is no need to zip the release again, as helm install kubecf/ is able to install the unpacked release.\nNote further that the above is an example of how to use the first kind of customization feature noted in the main README.\nIntegrating the Release in Kubecf With Quarks and Kubecf, BOSH releases can largely be used just the same as with a BOSH director. There are a few things Quarks offers, however, to make the adaptation to the Kubernetes environment easier.\nBPM BPM configurations for jobs are parsed from a rendered bpm.yml, as usual. But if need be, it is also possible to override the BPM configuration in the deployment manifest in the quarks field. See the bpm documentation for details on how to configure BPM.\nExample:\ninstance_groups:-name:natsinstances:2jobs:-name:natsproperties:quarks:bpm:processes:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\"/var/vcap/jobs/nats/config/nats.conf\" Note: The next section on ops files explains how this can be applied without the need to modify the original deployment manifest using ops files.\nOperation Files ops files can be used to modify arbitrary parts of the deployment manifest before it is applied. To do so, create a file in the directory deploy/helm/scf/assets/operations/instance_groups and it will be automagically applied during installation, courtesy of the bazel machinery.\nThe ops file for the example above could look like this:\n-type:replacepath:/instance_groups/name=nats/jobs/name=nats/properties/quarks?/bpm/processesvalue:-name:natslimits:open_files:50executable:/var/vcap/packages/gnatsd/bin/gnatsdargs:--c-\"/var/vcap/jobs/nats/config/nats.conf\" Testing With Kubecf After upload and integration, it is possible to build and deploy Kubecf according to any of the recipes listed by the main README.\n","excerpt":" Table Of Contents  Preparing the Release Image  Building a Docker …","ref":"/docs/tutorials/bosh-integration/","title":"Bosh releases integration"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf locally using:\n Minikube to manage a local Kubernetes cluster. A cf-operator pinned with Bazel. Kubecf built and deployed from the sources in the current checkout.  Minikube Minikube is one of several projects enabling the deployment, management and tear-down of a local Kubernetes cluster.\nThe Kubecf Bazel workspace contains targets to deploy and/or tear-down a Minikube-based cluster. Using these has the advantage of using a specific version of Minikube. On the other side, the reduced variability of the development environment is a disadvantage as well, possibly allowing portability issues to slide through.\n   Operation Command     Deployment bazel run //dev/minikube:start   Tear-down bazel run //dev/minikube:delete    Attention, Dangers Minikube edits the Kubernetes configuration file referenced by the environment variable KUBECONFIG, or ~/.kube/config.\nTo preserve the original configuration either make a backup of the relevant file, or change KUBECONFIG to a different path specific to the intended deployment.\nAdvanced configuration The local Minikube Documentation explains the various environment variables which can be used to configure the resources used by the cluster (CPUs, memory, disk size, etc.) in detail.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same kube cluster Kubecf will be deployed to.\nDeployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down cf-operator:\n   Operation Command     Deployment bazel run //dev/cf_operator:apply   Tear-down bazel run //dev/cf_operator:delete    Kubecf With all the prequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nSystem domain The main configuration to set for kubecf is its system domain. For the Minikube foundation we have to specify it as:\necho \"system_domain: $(minikube ip).xip.io\" \\  \u003e \"$(bazel info workspace)/dev/kubecf/system_domain_values.yaml\" Deployment and Tear-down The Kubecf Bazel workspace contains targets to deploy and/or tear-down kubecf from the sources:\n   Operation Command     Deployment bazel run //dev/kubecf:apply   Tear-down bazel run //dev/kubecf:delete    In this default deployment kubecf is launched without Ingress, and uses the Diego scheduler.\nAccess Accessing the cluster from outside of the minikube VM requires ingress to be set up correctly.\nTo access the cluster after the cf-operator has completed the deployment and all pods are active invoke:\ncf api --skip-ssl-validation \"https://api.$(minikube ip).xip.io\" # Copy the admin cluster password. acp=$(kubectl get secret \\  --namespace kubecf kubecf.var-cf-admin-password \\  -o jsonpath='{.data.password}' \\  | base64 --decode) # Use the password from the previous step when requested. cf auth admin \"${acp}\" Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, add a file matching the pattern *values.yaml to the directory dev/kubecf and containing\nfeatures:eirini:enabled:true before deploying kubecf.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \"tcp.2222=kubecf/kubecf-scheduler:2222\" \\  --set \"tcp.\u003cservices.tcp-router.port_range.start\u003e=kubecf/kubecf-tcp-router:\u003cservices.tcp-router.port_range.start\u003e\" \\  ... --set \"tcp.\u003cservices.tcp-router.port_range.end\u003e=kubecf/kubecf-tcp-router:\u003cservices.tcp-router.port_range.end\u003e\" \\  --set \"controller.service.externalIPs={$(minikube ip)}\" The tcp.\u003cport\u003e option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nThe last flag in the command above assigns the external IP of the cluster to the Ingress Controller service.\nConfigure kubecf Place a file matching the pattern *values.yaml into the directory dev/kubecf and containing\nfeatures:ingress:enabled:true","excerpt":" The intended audience of this document are developers wishing to …","ref":"/docs/tutorials/deploy-minikube/","title":"Deploy KubeCF in Minikube"},{"body":"","excerpt":"","ref":"/docs/concepts/","title":"Concepts"},{"body":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator enables the deployment of BOSH Releases, especially Cloud Foundry, to Kubernetes.\nIt’s implemented as a k8s operator, an active controller component which acts upon custom k8s resources.\n Installation notes Incubation Proposal: Containerizing Cloud Foundry Slack: #quarks-dev on https://slack.cloudfoundry.org Backlog: Pivotal Tracker Docker: https://hub.docker.com/r/cfcontainerization/cf-operator/tags  ","excerpt":" The CF-Operator is a Cloud Foundry Incubating Project.\n cf-operator …","ref":"/docs/concepts/operator/","title":"Cloud Foundry Operator"},{"body":"Project Quarks is an incubating effort within the Cloud Foundry Foundation that packages Cloud Foundry Application Runtime as containers instead of virtual machines, enabling easy deployment to Kubernetes.\nThe resulting containerized CFAR provides an identical developer experience to that of BOSH-managed Cloud Foundry installations, requires less infrastructure capacity and delivers an operational experience that is familiar to Kubernetes operators.\n","excerpt":"Project Quarks is an incubating effort within the Cloud Foundry …","ref":"/docs/concepts/quarks/","title":"Project Quarks"},{"body":" Once you have deployed KubeCF, you might want to validate it by running Cloud Foundry’s smoke tests. For that purpose, KubeCF’s helm chart ships the Cloud Foundry smoke tests, packaged inside of a deployment’s instance group.\nTriggering the smoke tests The smoke tests are run by a CF-Operator qjob (a wrapper on Kube jobs). That job is defined to not trigger automatically by default. To start it, patch the job with trigger strategy “now”:\n$ kubectl get qjob --namespace scf --output name 2\u003e /dev/null | grep smoke-tests quarksjob.quarks.cloudfoundry.org/smoke-tests $ kubectl patch quarksjob.quarks.cloudfoundry.org/smoke-tests \\ --namespace kubecf --type merge --patch \\ '{ \"spec\": { \"trigger\": { \"strategy\": \"now\" } } }'  This will start the job, which creates a smoke-tests-\u003cid\u003e pod. Inside that pod, there’s a container called smoke-tests-smoke-tests with the test run.\nYou can, as usual, see the resulting logs from the smoke-tests pod with:\n$ kubectl logs -f smoke-tests-614496c133797980-bm2g4 --namespace scf \\ --container smoke-tests-smoke-tests Running smoke tests... Running binaries smoke/isolation_segments/isolation_segments.test smoke/logging/logging.test smoke/runtime/runtime.test [1592406628] CF-Isolation-Segment-Smoke-Tests - 4 specs - 7 nodes SSSS SUCCESS! 13.717155722s [1592406628] CF-Logging-Smoke-Tests - 2 specs - 7 nodes S• SUCCESS! 36.291956367s [1592406628] CF-Runtime-Smoke-Tests - 2 specs - 7 nodes S• SUCCESS! 30.456607562s Ginkgo ran 3 suites in 1m21.517660359s Test Suite Passed  The pod will exit with a return code of 0 if successful, and other if not.\n","excerpt":" Once you have deployed KubeCF, you might want to validate it by …","ref":"/docs/tutorials/run-smoke-tests/","title":"Run smoke tests"},{"body":" Upgrading from a previous deployment Upgrading is roughly the same as doing an initial deployment; however, please use helm upgrade intead of helm install.\nIf you are providing a configuration file that was originally from a previous deployment, please take care to review the configuration to ensure you are not maintaining default values from a previous version unintentionally. Where possible, not specifying configuration that maintains the default values will prevent accidentally changing them if the defaults have changed in the new version.\nUpgrading from SCF Please refer to the SUSE documentation. Exporting data from the SCF insallation and importing into KubeCF is required.\n","excerpt":" Upgrading from a previous deployment Upgrading is roughly the same as …","ref":"/docs/tasks/upgrade/","title":"Upgrading KubeCF deployments"},{"body":" Deploying on RHEL / CentOS 7 -based Kubernetes If you are deploying with diego (that is, Eirini is not enabled) on top of a RHEL / CentOS 7 based cluster, please make sure that the user.max_user_namespaces sysctl is set to a large number. Do this on any worker node that may host any diego-cell workloads:\nsudo sh -c 'sysctl -w user.max_user_namespaces=15076 | tee -a /etc/sysctl.conf' This is not necessary on RHEL / CentOS 8 based systems.\n","excerpt":" Deploying on RHEL / CentOS 7 -based Kubernetes If you are deploying …","ref":"/docs/tasks/deploy/","title":"Deploy KubeCF"},{"body":" Rotating secrets is in general the process of updating one or more secrets to new values and restarting all affected pods so that they will use these new values.\nMost of the process is automatic. How to trigger it is explained in the following document.\nBeyond this, the keys used to encrypt the Cloud Controller Database (CCDB) can also be rotated, however, they do not exist as general secrets of the KubeCF deployment. This means that the general process explained above does not apply to them.\nThe audience of this document are:\n Developers working on KubeCF.\n Operators deploying KubeCF.\n  Background One of the features KubeCF (or rather the cf-operator it sits on top of) provides is the ability to declare secrets (passwords and certificates) and have the system automatically generate something suitably random for such on deployment, and distribute the results to the pods using them.\nThis removes the burden from human operators to come up with lots of such just to have all the internal components of KubeCF properly wired up for secure communication.\nHowever, even with this, operators may wish to change such secrets from time to time, or on a schedule. In other words, re-randomize the board, and limit the lifetime of any particular secret.\nAs a note on terminology, this kind of change is called rotating a secret.\nThis document describes how this can be done, in the context of KubeCF.\nFinding secrets Retrieve the list of all secrets maintained by a KubeCF deployment via\nkubectl get quarkssecret --namespace kubecf  To see the information about a specific secret, for example the NATS password, use\nkubectl get quarkssecret --namespace kubecf kubecf.var-nats-password --output yaml  Note that each quarkssecret has a corresponding regulare k8s secret it controls.\nkubectl get secret --namespace kubecf kubectl get secret --namespace kubecf kubecf.var-nats-password --output yaml  Requesting a rotation for a specific secret We keep using kubecf.var-nats-password as our example secret.\nTo rotate this secret:\n Create a YAML file for a ConfigMap of the form:\n --- apiVersion: v1 kind: ConfigMap metadata: name: rotate-kubecf.var-nats-password labels: quarks.cloudfoundry.org/secret-rotation: \"true\" data: secrets: '[\"kubecf.var-nats-password\"]'  Note, while the name of this ConfigMap can be technically anything (allowed by k8s syntax) we recommend using a name derived from the name of the secret itself, to make the connection clear.\nNote further that while this example rotates only a single secret, the data.secrets key accepts an array of secret names, allowing the simultaneous rotation of many secrets together.\n Apply this ConfigMap using:\n kubectl apply --namespace kubecf -f /path/to/your/yaml/file  The cf-operator will process this ConfigMap due the label\n quarks.cloudfoundry.org/secret-rotation: \"true\"  and knows that it has to invoke a rotation of the referenced secrets.\nThe actions of the cf-operator can be followed in its log.\n After the cf-operator has done the rotation, i.e. has not only changed the secrets, but also restarted all affected pods (the users of the rotated secrets), delete the trigger config map again:\nkubectl delete –namespace kubecf -f /path/to/your/yaml/file\n   Rotating the CCDB encryption keys IMPORTANT - Always backup the database before rotating the encryption key.\nThe key used to encrypt the database is generated the first time kubecf is deployed. It is based on the Helm values:\nccdb:encryption:rotation:key_labels:-encryption_key_0current_key_label:encryption_key_0 For each label under key_labels, kubecf will generate an encryption key. The current_key_label indicates which key is currently being used.\nIn order to rotate the CCDB encryption key, add a new label to key_labels (keeping the old labels), and mark the current_key_label with the newly added label. Example:\nccdb:encryption:rotation:key_labels:-encryption_key_0-encryption_key_1current_key_label:encryption_key_1 IMPORTANT - key labels should be less than 240 characters long.\nThen, update the kubecf Helm installation. After Helm finishes its updates, trigger the rotate-cc-database-key errand:\nNote - the following command assumes the Helm installation is named kubecf and it was installed to the kubecf namespace. These values may be different depending on how kubecf was installed.\nkubectl patch qjob kubecf-rotate-cc-database-key \\  --namespace kubecf \\  --type merge \\  --patch '{\"spec\":{\"trigger\":{\"strategy\":\"now\"}}}'","excerpt":" Rotating secrets is in general the process of updating one or more …","ref":"/docs/tasks/secrets/","title":"Secret rotation KubeCF"},{"body":"","excerpt":"","ref":"/docs/tasks/","title":"Core Tasks"},{"body":"","excerpt":"","ref":"/docs/tasks/bosh/","title":"Test BOSH releases"},{"body":"","excerpt":"","ref":"/docs/tutorials/","title":"Tutorials"},{"body":"To see the docs referring to internals or specific code areas of KubeCF, we recommend looking at: https://github.com/cloudfoundry-incubator/kubecf/tree/master/doc\nA good starting point is the doc’s Contribution guide, and its directory organization table.\n","excerpt":"To see the docs referring to internals or specific code areas of …","ref":"/docs/reference/","title":"Reference"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nHere we explain how to deploy Kubecf using:\n A generic kubernetes cluster. A released cf-operator helm chart. A released kubecf helm chart.  Before To assure that kubecf is installed with the correct cf-operator version, it’s highly recommended to use the kubecf bundle artifact from the GitHub releases page.\nKubernetes In contrast to other instructions, we are not set on using a local cluster. Any Kubernetes cluster will do, assuming that the following requirements are met:\n Presence of a default storage class (provisioner).\n For use with a diego-based kubecf (default), a node OS with XFS support.\n For GKE, using the option --image-type UBUNTU with the gcloud beta container command selects such an OS.   This can be any of, but is not restricted to:\n GKE AKS EKS  Note that how to deploy and tear-down such a cluster is outside of the scope of instructions.\ncf-operator The cf-operator is the underlying generic tool to deploy a (modified) BOSH deployment like Kubecf for use.\nIt has to be installed in the same Kubernetes cluster that Kubecf will be deployed to.\nHere we are not using development-specific dependencies like bazel, but only generic tools, i.e. kubectl and helm.\nInstalling and configuring Helm is the same regardless of the chosen foundation, and assuming that the cluster does not come with Helm Tiller pre-installed.\nDeployment First, let’s create the cf-operator namespace manually\n$ kubectl create namespace cf-operator namespace/cf-operator then we can proceed with the cf-operator deployment\n$ helm install cf-operator \\ --namespace cf-operator \\ --set \"global.singleNamespace.name=kubecf\" \\ ./cf-operator.tgz NAME: cf-operator LAST DEPLOYED: Mon Sep 28 11:30:32 2020 NAMESPACE: cf-operator STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Running the operator will install the following CRD´s: - boshdeployments.quarks.cloudfoundry.org - quarksjobs.quarks.cloudfoundry.org - quarksecrets.quarks.cloudfoundry.org - quarkstatefulsets.quarks.cloudfoundry.org You can always verify if the CRD´s are installed, by running: $ kubectl get crds You can check the charts README: `helm show readme quarks/cf-operator` for more information about configuration options. Interacting with the cf-operator pod 1. Check the cf-operator pod status kubectl -n cf-operator get pods 2. Tail the cf-operator pod logs export OPERATOR_POD=$(kubectl get pods -l name=cf-operator --namespace cf-operator --output name) kubectl -n cf-operator logs $OPERATOR_POD -f 3. Apply one of the BOSH deployment manifest examples kubectl -n kubecf apply -f docs/examples/bosh-deployment/boshdeployment-with-custom-variable.yaml 4. See the cf-operator in action! watch -c \"kubectl -n kubecf get pods\" and wait\n$ watch -c kubectl -n cf-operator get pods Every 2.0s: kubectl -n cf-operator get pods Jaimes-MacBook-Pro.local: Mon Sep 28 11:31:04 2020 NAME READY STATUS RESTARTS AGE cf-operator-c89644498-q75dh 1/1 Running 0 30s cf-operator-quarks-job-85665697bb-72vsp 1/1 Running 0 31s cf-operator-quarks-secret-844844556b-xl9jq 1/1 Running 0 31s until all the pods are up \u0026 running.\nNote: \u003e The above helm install will generate many controllers spread over multiple pods inside the cfo \u003e namespace. Most of these controllers run inside the cf-operator pod. \u003e \u003e The global.singleNamespace.name=kubecf path tells the controllers to watch for CRD´s instances into the kubecf namespace. \u003e \u003e The cf-operator helm chart will generate the kubecf namespace during installation, and \u003e eventually one of the controllers will use a webhook to label this namespace with the \u003e cf-operator-ns key. \u003e \u003e If the kubecf namespace is deleted, but the operators are still running, they will no longer \u003e know which namespace to watch. This can lead to problems, so make sure you also delete the pods \u003e inside the cfo namespace, after deleting the kubecf namespace.\nNote: \u003e how the namespace the operator is installed into (cfo) differs from the namespace the operator \u003e is watching for deployments (kubecf). \u003e This form of deployment enables restarting the operator because it is not affected by webhooks. \u003e It further enables the deletion of the Kubecf deployment namespace to start from scratch, without \u003e redeploying the operator itself.\nTear-down $ helm uninstall cf-operator --namespace cf-operator release \"cf-operator\" uninstalled KubeCF With all the prerequisites handled by the preceding sections it is now possible to build and deploy kubecf itself.\nThis again uses helm and a released helm chart.\nDeployment $ helm install kubecf \\ --namespace kubecf \\ --set \"system_domain=kubecf.yourdomain.net\" \\ ./kubecf_release.tgz NAME: kubecf LAST DEPLOYED: Mon Sep 28 11:39:01 2020 NAMESPACE: kubecf STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Welcome to your new deployment of KubeCF. The endpoint for use by the `cf` client is https://api.kubecf.suse.dev To target this endpoint and login, run cf login --skip-ssl-validation -a https://api.kubecf.suse.dev -u admin Please remember, it may take some time for everything to come online. You can use kubectl get pods --namespace kubecf to spot-check if everything is up and running, or watch -c 'kubectl get pods --namespace kubecf' to monitor continuously. You will be running 1 diego cell(s), with 40960Mi of disk each. The default app quota is set to 1024Mi, which means you'll have enough disk space to run about 40 apps. The online documentation (release notes, deployment guide) can be found at https://kubecf.io/docs and wait until all the pods are up \u0026 running (which can take around 20 minutes)\n$ watch -c 'kubectl get pods --namespace kubecf' Every 2.0s: kubectl get pods --namespace kubecf Jaimes-MacBook-Pro.local: Mon Sep 28 12:03:23 2020 NAME READY STATUS RESTARTS AGE api-0 17/17 Running 1 16m apps-dns-77b46d5657-5rm24 1/1 Running 0 24m auctioneer-0 6/6 Running 1 16m bosh-dns-8679ff8bd4-5hb95 1/1 Running 0 22m bosh-dns-8679ff8bd4-fzkhq 1/1 Running 0 22m cc-worker-0 6/6 Running 0 16m credhub-0 8/8 Running 2 16m database-0 2/2 Running 0 22m database-seeder-25b239b49d162e7c-xhrmr 0/2 Completed 0 23m diego-api-0 9/9 Running 2 16m diego-cell-0 12/12 Running 2 16m doppler-0 6/6 Running 0 16m log-api-0 9/9 Running 0 16m log-cache-0 10/10 Running 0 16m nats-0 7/7 Running 0 16m router-0 7/7 Running 0 16m routing-api-0 6/6 Running 0 16m scheduler-0 13/13 Running 1 16m singleton-blobstore-0 8/8 Running 0 16m tcp-router-0 7/7 Running 0 16m uaa-0 9/9 Running 4 16m In this default deployment, kubecf is launched without Ingress, and it uses the Diego scheduler.\nTear-down $ helm uninstall kubecf --namespace kubecf release \"kubecf\" uninstalled Access Run the following commands to access the cluster after the cf-operator and KubeCF has completed sucessfully the deployment and all pods are up and running correctly\ncf api --skip-ssl-validation \"https://api.\u003cdomain\u003e\" Copy the admin cluster password.\n$ admin_pass=$(kubectl get secret \\  --namespace kubecf var-cf-admin-password \\  -o jsonpath='{.data.password}' \\  | base64 --decode) Use the password from the previous step when requested.\ncf auth admin \"${admin_pass}\" Advanced Topics Diego vs Eirini Diego is the standard scheduler used by kubecf to deploy CF applications. Eirini is an alternative to Diego that follows a more Kubernetes native approach, deploying the CF apps directly to a Kubernetes namespace.\nTo activate this alternative, use the option --set features.eirini.enabled=true when deploying kubecf from its chart.\nDiego Cell Affinities \u0026 Tainted Nodes Note that the diego-cell pods used by the Diego standard scheduler are\n privileged, use large local emptyDir volumes (i.e. require node disk storage), and set kernel parameters on the node.  These things all mean that these pods should not live next to other Kubernetes workloads. They should all be placed on their own dedicated nodes instead where possible.\nThis can be done by setting affinities and tolerations, as explained in the associated tutorial.\nIngress By default, the cluster is exposed through its Kubernetes services.\nTo use the NGINX ingress instead, it is necessary to:\n Install and configure the NGINX Ingress Controller. Configure Kubecf to use the ingress controller.  This has to happen before deploying kubecf.\nInstallation of the NGINX Ingress Controller helm install stable/nginx-ingress \\  --name ingress \\  --namespace ingress \\  --set \"tcp.2222=kubecf/scheduler:2222\" \\  --set \"tcp.\u003cservices.tcp-router.port_range.start\u003e=kubecf/tcp-router:\u003cservices.tcp-router.port_range.start\u003e\" \\  ... --set \"tcp.\u003cservices.tcp-router.port_range.end\u003e=kubecf/tcp-router:\u003cservices.tcp-router.port_range.end\u003e\" The tcp.\u003cport\u003e option uses the NGINX TCP pass-through.\nIn the case of the tcp-router ports, one --set for each port is required, starting with services.tcp-router.port_range.start and ending with services.tcp-router.port_range.end. Those values are defined on the values.yaml file with default values.\nConfigure kubecf Use the Helm option --set features.ingress.enabled=true when deploying kubecf.\nExternal Database By default, KubeCF includes an internal single-availability database. KubeCF also exposes a way to use an external database via the Helm property features.external_database. Check the values.yaml for more details.\nSetting features.external_database.seed to true will allow KubeCF to configure the various required databases for you; however, this will require providing the root database password (via the var-pxc-root-password secret), which may be an issue under some policies.\nIt is also possible to manually create all necessary databases externally, and then provide the individual credentials to KubeCF. When deploying KubeCF with Helm, it will error out if any of the required database credentials are missing. An example configuration is below:\nfeatures:external_database:enabled:truetype:mysqlhost:mariadb-server.corp.example.comport:3306seed:falsedatabases:uaa:name:uaausername:uaa-database-userpassword:PAWjxQst5l16J3w3vJdfX7fXY8HyHtybcc:name:cloud_controllerusername:cc-database-userpassword:qj5KI3qwVe+XVFENAuEU9AfSkpUK/nzbbbs:name:diegousername:diego-database-userpassword:z7gvkjWNUgX+xcn3Ia0loMEnAD7MXBgErouting_api:name:routing-apiusername:routing-api-database-userpassword:bTL6DhK89F+G05OZtHbvEdR1uwkyRMJtpolicy_server:name:network_policyusername:network-policy-database-userpassword:EYviLFS/F4dAyVry5Stm8wrpOI64Xmnzsilk_controller:name:network_connectivityusername:silk-database-userpassword:ah7nbU1wsHuZ4BtmcdU1vV37KgVV2lLflocket:name:locketusername:locket-database-userpassword:rWicBxhg8mIrhkuSR/aDlqljOMORuyLcredhub:name:credhubusername:credhub-database-userpassword:5tJcIWHCR1QTLdfQDiN1Mz8K8jB+clDautoscaler:name:autoscalerusername:autoscaler-database-userpassword:cYo2Tdi61N39mUjoEksXHuu3F7Dk58F","excerpt":" The intended audience of this document are developers wishing to …","ref":"/docs/deployment/kubernetes-deploy/","title":"Deployment Walkthrough"},{"body":" The intended audience of this document are developers wishing to contribute to the Kubecf project.\nIt provides a basic overview of various aspects of the project below, and uses these overviews as the launching points to other documents which go deeper into the details of each aspect.\n Table of Contents (Aspects)  Deployment Pull Requests Source Organization Updating Subcharts Docker Images Linting Patching [BOSH Development Workflow] Rotating secrets  Deployment Kubecf is built on top of a number of technologies, namely Kubernetes, Helm (charts), and the cf-operator.\nFor all these we have multiple choices for installing them, and various interactions between the choices influence the details of the commands to use.\nInstead of trying to document all the possibilities and all their interactions at once, supporting documents will describe specific combinations of choices in detail, from the bottom up.\n   Document Description     Local Minikube Minikube + Operator + Kubecf   General Kube Any Kube + Operator/Helm + Kubecf/Helm    Pull Requests The general work flow for pull requests contributing bug fixes, features, etc. is:\n Branch or Fork the cloudfoundry-incubator/kubecf repository, depending on permissions.\n Implement the bug fix, feature, etc. on that branch/fork.\n Submit a pull request based on the branch/fork through the github web interface, against the master branch.\n Developers will review the content of the pull request, asking questions, requesting changes, and generally discussing the submission with the submitter and among themselves.\n PRs from branches of this repository are automatically tested in the CI. For forks, you should ask a maintainer of this repository to trigger a build. Automated triggers have been disabled for security reasons.\n After all issues with the request are resolved, and CI has passed, a developer will merge it into master.\n Note that it may be necessary to rebase the branch/fork to resolve any conflicts due to other PRs getting merging while the PR is under discussion.\nSuch a rebase will be a change request from the developers to the contributor, on the assumption that the contributor is best suited to resolving the conflicts.\n  Source Organization The important directories of the kubecf sources, and their contents are shown in the table below. Each directory entry links to the associated documentation, if we have any.\n   Directory Content     top Documentation entrypoint, License,    Main workspace definitions.   top/…/README.md Directory-specific local documentation.   top/bosh/releases Support for runtime patches of a kubecf deployment.   top/doc Global documentation.   top/dev/cf_deployment/bump Tools to support updating the cf deployment    manifest used by kubecf.   top/dev/cf_cli Deploy cf cli into a helper pod from which to then    inspect the deployed Kubecf   top/dev/kube Tools to inspect kube clusters and kubecf deployments.   top/dev/kubecf Kubecf chart configuration   top/deploy/helm/kubecf Templates and assets wrapping a CF deployment    manifest into a helm chart.   top/rules Supporting scripts.   top/testing Scripts with specific testing   top/scripts Developer scripts used by make to start a k8s cluster    (for example on kind), lint, build, run \u0026 test kubecf   top/scripts/tools Developer scripts pinning the development dependencies    Updating subcharts The kubecf helm chart includes a number of subcharts. They are declared in requirements.yaml. For the convenience of development they are included in unpacked form directly in this repo, so version changes can be inspected with regular git tools, and the subcharts can be searched with grep etc.\nThe procedure to update the version of a subchart is:\nvi deploy/helm/kubecf/requirements.yaml ./dev/helm/update_subcharts.sh git commit  Docker Images The docker images used by kubecf to run jobs in container use a moderately complex naming scheme.\nThis scheme is explained in a separate document: The Naming Of Docker Images in kubecf.\nLinting Currently, 3 linters are available: shellcheck, yamllint, \u0026 helm linting.\nInvoke these linters with\nmake lint to run shellcheck on all .sh files found in the entire checkout, or yamllint on all .yaml or .yml files respectively, and report any issues found. The last option runs helm lint (without --strict) on the generated helm chart.\nSee the authoritative list of linters being called in the make lint target.\nPatching Background The main goal of the CF operator is to take a BOSH deployment manifest, deploy it, and have it run as-is.\nNaturally, in practice, this goal is not quite reached yet, requiring patching of the deployment manifest in question, and/or the involved releases, at various points of the deployment process. The reason behind a patch is generally fixing a problem, whether it be from the translation into the kube environment, an issue with an underlying component, or something else.\nThen, there are features, given the user of the helm chart wrapped around the deployment manifest the ability to easily toggle various preset configurations, for example the use of eirini instead of diego as the application scheduler.\nFeatures A feature of kubecf is usually implemented using a combination of Helm templating and BOSH ops files.\nThe helm templating is used to translate the properties in the chart’s values.yaml to the actual actions to take, by including/excluding chart elements, often the BOSH ops files containing the structured patches modifying the deployment itself (changing properties, adding/removing releases, (de)activating jobs, etc.)\nThe helm templating is applied when the kubecf chart is deployed.\nThe ops files are then applied by the operator, transforming the base manifest from the chart into the final manifest to deploy.\nCustomization Kubecf provides two mechanisms for customization during development (and maybe by operators ?):\n The property .Values.operations.custom of the chart is a list of names for kube configmaps containing the texts of the ops files to apply beyond the ops files from the chart itself.\nNote that we are talking here about a yaml structure whose data.ops property is a text block holding the yaml structure of an ops file.\nThere is no tooling to help the writer with the ensuing quoting hell.\nNote further that the resulting config maps have to be applied, i.e. uploaded into the kube cluster before deploying the kubecf helm chart with its modified values.yaml.\nFor example, kubectl apply the object below\n ```yaml --- apiVersion: v1 kind: ConfigMap metadata: name: configmap_name data: ops: |- some_random_ops ```  and then use\n ```yaml operations: custom: - configmap_name ```  in the values.yaml (or an equivalent --set option) as part of a kubecf deployment to include that ops file in the deployment.\nThe [BOSH Development Workflow] is an example of its use.\n[BOSH Development Workflow]: bosh-release-development.md\n The second mechanism allows the specification of any custom BOSH property for any instancegroup and job therein.\nJust specifying\n ```yaml properties: instance-group-name: job-name: some-property: some-value ```  in the values.yaml for the kubecf chart causes the chart to generate and use an ops file which applies the assignment of some-value to some-property to the specified instance group and job during deployment.\nAn example of its use in Kubecf is limiting the set of test suites executed by the CF acceptance tests.\n  Both forms of customization assume a great deal of familiarity on the part of the developer and/or operator with the BOSH releases, instance groups and jobs underlying the CF deployment manifest, i.e. which properties exist, what changes to them mean and how they affect the system.\nPatches In SCF v2, the predecessor to kubecf, the patches scripts enabled developers and maintainers to apply general patches to the sources of a job (i.e. configuration templates, script sources, etc.) before that job was rendered and then executed. At the core, the feature allows the user to execute custom scripts during runtime of the job container for a specific instance_group.\nPre render scripts are the equivalent feature of the CF operator.\nKubecf makes use of this feature to fix a number of issues in the deployment. The relevant patch scripts are found under the directory bosh/releases/pre_render_scripts.\nWhen following the directory structure explained by the README, the bazel machinery for generating the kubecf helm chart will automatically convert these scripts into the proper ops files for use by the CF operator.\nAttention All patch scripts must be idempotent. In other words, it must be possible to apply them multiple times without error and without changing the result.\nThe existing patch scripts do this by checking if the patch is already applied before attempting to apply it for real.\nRotating Secrets Rotating secrets is in general the process of updating one or more secrets to new values and restarting all affected pods so that they will use these new values.\nMost of the process is automatic. How to trigger it is explained in Secret Rotation.\nBeyond this, the keys used to encrypt the Cloud Controller Database (CCDB) can also be rotated, however, they do not exist as general secrets of the KubeCF deployment. This means that the general process explained above does not apply to them.\nTheir custom process is explained in CCDB encryption key rotation.\n","excerpt":" The intended audience of this document are developers wishing to …","ref":"/docs/contribution-guidelines/","title":"Contribution Guidelines"},{"body":" KubeCF is under active development. Note there might be discrepancies between the docs and latest releases.\n ","excerpt":" KubeCF is under active development. Note there might be discrepancies …","ref":"/docs/","title":"Documentation"},{"body":" This document describes how the administrator may influence how the various workloads that are part of KubeCF will get deployed onto their Kuberntes cluster.\nAffinities Kubernetes will attempt to schedule work based on their affinities and anti-affinities; by default, KubeCF will request for each instance group to be scheduled away from other replicas of the same instance group. Additionally, by default the router instance group and the diego-cell instance group will have an anti-affinity towards each other.\nThe affinities can be overridden on a per-instance-group basis, using helm configuration values such as the following:\nsizing:uaa:affinity:podAntiAffinity:preferredDuringSchedulingIgnoredDuringExecution:-labelSelector:matchExpressions:-key:quarks.cloudfoundry.org/quarks-statefulset-nameoperator:Invalues:-uaa-api This will request uaa pods to avoid any other uaa or api pods; this may be helpful if your UAA instances consume resources to the point where they slow down API access. Note that if any affinity or anti-affinity options are given, they will override the default anti-affinities; it is recommended that they be specified explicitly as well, as given in the example above.\nNote that it is also possible to declare nodeAffinity and podAffinity, as the whole affinity block is assumed to be a valid Kubernetes affinity block.\nTolerations Kubernetes has a concept of taints and tolerations, which can be used to prevent workloads from running on a given node, and then whitelist some workloads on it again; this can be used to do things such as ensuring the physical host has the appropriate kinds of resources, or to evict work from nodes that will be removed.\nTolerations can be configured in KubeCF on an instance group level, by providing the appropriate configuration in the helm values. For example, to allocate a Kubernetes node such that it will only run digeo-cell, we could do:\n# This marks the node \"beefy-node\" with a taint of \"instance-group\" # with a value of \"diego-cell\", and prevents scheduling workloads that # do not have a matching taint. kubectl taint nodes beefy-node instance-group=diego-cell:NoSchedule We could then allow diego-cell to be scheduled onto that node by deploying KubeCF with a helm values.yaml that contains the toleration:\nsizing:diego-cell:tolerations:-key:instance-groupoperator:Equalvalue:diego-celleffect:NoSchedule Then any diego-cell containers will be allowed to run on the beefy-node node. This, of course, does not guarantee that those containers will actually run on that node; it is also available to run on any other node. In order to enforce placement, we will also need to add a node label:\nkubectl label nodes beefy-node instance-group-label=diego-cell It is then possible to add a node affinity term to the instane group, so that it will always be scheduled on nodes with the given label:\nsizing:diego-cell:# tolerations: as aboveaffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:-matchExpressions:-key:instance-group-labeloperator:Invalues:-diego-cell# podAntiAffinity term, as above; the defaults will be lost.","excerpt":" This document describes how the administrator may influence how the …","ref":"/docs/deployment/affinities-and-tolerations/","title":"Kubernetes Affinities and Tolerations"},{"body":"","excerpt":"","ref":"/index.json","title":""},{"body":" KubeCF community This is the place to start your journey with KubeCF and hang out with the KubeCF community!\nThe following section will provide useful links to become a KubeCF contributor, and guide you to the first steps into KubeCF.\nCode of Conduct Important Before going ahead, please review the KubeCF Code of Conduct.\nKubeCF follows the CloudFoundry Foundation Code of Conduct. Unacceptable behavior may be reported by contacting the CloudFoundry Foundation via conduct@cloudfoundry.org.\nUseful links  Documentation Contribution Guidelines Deploy locally with kind  Not sure where to start? have a quick look at the open issues that are marked as “Good first issues”\n","excerpt":" KubeCF community This is the place to start your journey with KubeCF …","ref":"/community/","title":"Community"},{"body":" Learn More   Download   A Kubernetes Native Distribution of Cloud Foundry\nKubeCF is a distribution of Cloud Foundry Application Runtime (CFAR) for Kubernetes. It gives developers the productivity they love from Cloud Foundry and allows platform operators to manage the infrastructure abstraction with Kubernetes tools and APIs.           Cloud Foundry built for Kubernetes (formerly SUSE/scf v3 branch). It makes use of the Cloud Foundry Operator, which is incubating under Project Quarks.       CFAR KubeCF produces builds of CFAR which can be deployed to Kubernetes with Helm and managed by the cf-operator.\n   BOSH KubeCF already consumes your BOSH release and packages it for Kubernetes\n   Integration cf-operator extends Kubernetes to understand BOSH\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n    ","excerpt":" Learn More   Download   A Kubernetes Native Distribution of Cloud …","ref":"/","title":"KubeCF"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"}]